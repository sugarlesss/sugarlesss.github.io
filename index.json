[{"categories":null,"content":"Java 系列 Java 基础 Java 面向对象 Java 常用类 Java 多线程 Java TCP\u0026UDP ","date":"2021-07-19","objectID":"/special.html:0:0","series":null,"tags":null,"title":"专栏","uri":"/special.html"},{"categories":["折腾","Python"],"content":"长时间的电话 / 会议录音 / 演讲视频之类的各种音视频媒体文件的信息，如果有长期存储的需求的话，你可能会因为体积积太大，不方便进行信息检索而头疼。但如果转成文字，润色后整理成文字稿，无论是从存储体积或是信息检索方面，都会比直接存储音视频媒体更方便些。 尝遍了市面上各种音频转文字的野鸡产品后，我最终还是选择用大厂提供的 API ，to B 的产品在价格和可定制性上肯定比 to C 的产品更有优势。 关于如何将视频转音频，手段很多，比如使用 FFmpeg 可以将视频转为音频： ffmpeg -i input.mp4 -vn -acodec copy audio.mp3 一、音频转文字究竟有多贵？ 参考当前的大型互联网厂商提供的音频转文字服务定价，讯飞最贵、百度云论单价虽然最便宜，但起购门槛金额较高，比较适合商用走量、，腾讯云的价格最合适，而且最关键的是腾讯云每月都有附赠一定量的额度，可以无限白嫖。 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:0:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"1.1、讯飞 语音转写：文档地址 产品价格：文档地址 套餐一（ 20 小时） 168 元 / 单价 8.4 套餐二（ 200 小时） 980 元 / 单价 4.9 套餐三（ 1000 小时） 3900 元 / 单价 3.9 套餐四（ 3000 小时） 10500 元 / 单价 3.5 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:1:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"1.2、百度云 音频文件转写：文档地址 按小时包预付费：文档地址 按调用时长后付费：文档地址 按小时包预付费 套餐一（ 1000 小时） 1200 元 / 单价 1.2 套餐二（ 10000 小时） 9000 元 / 单价 0.9 套餐三（ 100000 小时） 70000 元 / 单价 0.7 套餐四（ 500000 小时） 300000 元 / 单价 0.6 按调用时长后付费 每小时 2 元，系统按用户实际使用，每小时出账单实时扣费，账户内需保留足量余额。 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:2:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"1.3、腾讯云 语音识别 ASR：https://cloud.tencent.com/product/asr 免费配额：https://console.cloud.tencent.com/asr/resourcebundle 资源包购买地址：https://buy.cloud.tencent.com/asr 录音文件识别（五小时内出结果） 套餐一（ 60 小时） 90 元 / 单价 1.5 套餐二（ 1000 小时） 1200 元 / 单价 1.2 套餐三（ 10000 小时） 10000 元 / 单价 1 套餐四（ 100000 小时） 80000 元 / 单价 0.8 套餐五（ 300000 小时） 210000 元 / 单价 0.7 支持中文普通话、英语、粤语、日语、泰语。对时长5小时以内的录音文件进行识别，异步返回识别全部结果。 支持语音 URL 和本地语音文件两种请求方式。 语音 URL 的音频时长不能长于5小时，文件大小不超过512MB。 本地语音文件不能大于5MB。 提交录音文件识别请求后，在5小时内完成识别（半小时内发送超过1000小时录音或者2万条识别任务的除外），识别结果在服务端可保存7天 支持回调或轮询的方式获取结果 录音文件识别极速版（准实时） 套餐一（ 30 小时） 72 元 / 单价 2.3 套餐二（ 1000 小时） 1500 元 / 单价 1.5 套餐三（ 10000 小时） 12000 元 / 单价 1.2 套餐四（ 100000 小时） 110000 元 / 单价 1.1 套餐五（ 300000 小时） 300000 元 / 单价 1 仅支持中文普通话，使用者通过 HTTPS POST 方式上传一段音频并在极短时间内同步返回识别结果，可满足音视频字幕、准实时质检等场景下对语音文件识别时效性的要求。 支持100MB以内音频文件的识别 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:3:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"1.4、阿里云 录音文件识别：文档地址 录音文件识别资源包：购买链接 录音文件识别（极速版）资源包：购买链接 录音文件识别（六小时内出结果） 套餐一（ 40 小时） 100 元 / 单价 2.5 套餐二（ 1000 小时） 1200 元 / 单价 1.2 套餐三（ 20000 小时） 20000 元 / 单价 1 套餐四（ 100000 小时） 90000 元 / 单价 0.9 套餐五（ 250000 小时） 200000 元 / 单价 0.8 录音文件识别极速版（30分钟以内时长的音频转写完成时间不超过10秒） 套餐一（ 40 小时） 90 元 / 单价 2.5 套餐二（ 1000 小时） 1560 元 / 单价 1.56 套餐三（ 20000 小时） 26000 元 / 单价 1.3 套餐四（ 100000 小时） 117000 元 / 单价 1.17 套餐五（ 250000 小时） 260000 元 / 单价 1.04 二、对接腾讯云 在使用腾讯云 API 之前，你需要先获取三个必要的参数。 在腾讯云控制台账号信息页面查看账号 APPID：https://console.cloud.tencent.com/developer 访问管理页面获取 SecretID 和 SecretKey：https://console.cloud.tencent.com/cam/capi ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:4:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.1、录音文件识别极速版（V2） 极速版演示，目前腾讯云给的免费配额是每月 5 小时。 极速版仅支持中文普通话，通过 HTTPS POST 方式上传一段音频并在极短时间内同步返回识别结果，可满足音视频字幕、准实时质检等场景下对语音文件识别时效性的要求。支持100MB以内音频文件的识别。 1）将真实的值填入下方源代码的 APPID、SECRET_ID、SECRET_KEY 这三个参数中； 2）将源代码中的 audio 参数修改为本地音频文件的相对或绝对路径； 3）run it。 # 录音文件识别极速版 v2版本 # API 文档：https://cloud.tencent.com/document/api/1093/52097 # 签名生成：https://cloud.tencent.com/document/api/1093/52097#sign import json import requests import time import hashlib import hmac import base64 # 调用者身份 class Credential: def __init__(self, app_id, secret_id, secret_key): self.app_id = app_id self.secret_id = secret_id self.secret_key = secret_key # 服务端信息 class Server: def __init__(self, protocol, host, port, uri): self.protocol = protocol self.host = host self.port = port self.uri = uri # 打印日志 def show(title=\"\", content=object, level=\"INFO\"): print(\"[\" + level + \"] \" + title) print(content) print() # 获取时间戳 def getTimestamp(): t = time.time() # return t # 原始时间数据 return int(t) # 秒级时间戳 # return int(round(t * 1000)) # 毫秒级时间戳 # return int(round(t * 1000000)) # 微秒级时间戳 # 字典序排序 def format_sign_string(param): param = sorted(param.items(), key=lambda d: d[0]) signstr = f\"POST{server.host + server.uri + credential.app_id}\" for t in param: if 'appid' in t: signstr += str(t[1]) break signstr += \"?\" for x in param: tmp = x if 'appid' in x: continue for t in tmp: signstr += str(t) signstr += \"=\" signstr = signstr[:-1] signstr += \"\u0026\" signstr = signstr[:-1] return signstr # 签名 def sign(signstr, secret_key): hmacstr = hmac.new(secret_key.encode('utf-8'), signstr.encode('utf-8'), hashlib.sha1).digest() s = base64.b64encode(hmacstr) s = s.decode('utf-8') return s # 调用者信息对象初始化 # 在腾讯云控制台账号信息页面查看账号 APPID：https://console.cloud.tencent.com/developer # 访问管理页面获取 SecretID 和 SecretKey：https://console.cloud.tencent.com/cam/capi APPID = \"\" SECRET_ID = \"\" SECRET_KEY = \"\" credential = Credential(APPID, SECRET_ID, SECRET_KEY) # 服务端信息对象初始化 PROTOCOL = \"https://\" HOST = \"asr.cloud.tencent.com\" PORT = \"\" URI = \"/asr/flash/v1/\" server = Server(PROTOCOL, HOST, PORT, URI) # 构造参数 params = { # 用户在腾讯云注册账号 AppId 对应的 SecretId \"secretid\": f\"{credential.secret_id}\", # 引擎模型类型。8k_zh：8k 中文普通话通用；16k_zh：16k 中文普通话通用；16k_zh_video：16k 音视频领域。 \"engine_type\": \"16k_zh\", # 音频格式。支持 wav、pcm、ogg-opus、speex、silk、mp3、m4a、aac。 \"voice_format\": \"mp3\", # 当前 UNIX 时间戳，如果与当前时间相差超过3分钟，会报签名失败错误。 \"timestamp\": getTimestamp(), } # 是否开启说话人分离（目前支持中文普通话引擎），默认为0，0：不开启，1：开启。 # params[\"speaker_diarization\"] = 0; # 是否过滤脏词（目前支持中文普通话引擎），默认为0。0：不过滤脏词；1：过滤脏词；2：将脏词替换为 *。 # params[\"filter_dirty\"] = 0; # 是否过滤语气词（目前支持中文普通话引擎），默认为0。0：不过滤语气词；1：部分过滤；2：严格过滤。 # params[\"filter_modal\"] = 0; # 是否过滤标点符号（目前支持中文普通话引擎），默认为0。0：不过滤，1：过滤句末标点，2：过滤所有标点。 # params[\"filter_punc\"] = 0; # 是否进行阿拉伯数字智能转换，默认为1。0：全部转为中文数字；1：根据场景智能转换为阿拉伯数字。 params[\"convert_num_mode\"] = 1; # 是否显示词级别时间戳，默认为0。0：不显示；1：显示，不包含标点时间戳，2：显示，包含标点时间戳。 # params[\"word_info\"] = 0; # 是否只识别首个声道，默认为1。0：识别所有声道；1：识别首个声道。 # params[\"first_channel_only\"] = 1; show(\"生成 params\", params) # 获取 signature signstr = format_sign_string(params) signature = sign(signstr, SECRET_KEY) show(\"生成 signstr\", signstr) show(\"生成 signature\", signature) # 计算 URL URL = server.protocol + signstr[4::] show(\"计算 URL\", URL) # 构造 header headers = { \"Host\": \"asr.cloud.tencent.com\", \"Authorization\": f\"{signature}\", # \"Content-Type\": \"application/octet-stream\", # \"Content-Length\": \"请求长度，此处对应语音数据字节数，单位：字节\" } show(\"生成 headers\", headers) # 音频路径 audio = \"./外卖佣金到底有多高.mp3\" with open(audio, 'rb') as f: # 读取音频数据 data = f.read() # 调用腾讯云语音识别 API e = requests.post(URL, headers=headers, data=data) responese = json.loads(e.text) code = responese[\"code\"] if code != 0: show(\"识别失败\", responese) else: show(\"识别成功\", responese) # 一个channl_result对应一个声道的识别结果 # 大多数音频是单声道，对应一个channl_result for channl_result in responese[\"flash_result\"]: channel_id = channl_result['channel_id'] text = channl_result['text'] show(f\"channel_id: {channel_id}\", text) 输出 [INFO] 生成 params {'secretid': 'xxxxxxxxxxxxxxxxxxx', 'engine_type': '16k_zh', 'voice_format': 'mp3', 'timestamp': xxxxxxxxx, 'convert","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:5:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.2、录音文件识别（V3） 录音文件识别演示，目前腾讯云给的免费配额是每月 10 小时。 录音文件识别请求 API 文档：https://cloud.tencent.com/document/api/1093/37823 录音文件识别结果查询 API 文档：https://cloud.tencent.com/document/api/1093/37822 腾讯云 API 3.0 提供了配套的开发工具集 SDK：https://cloud.tencent.com/document/api/1093/37823#SDK ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:6:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.2.1、安装 Python SDK 在使用录音文件识别时，需要通过 pip 方式安装腾讯云提供的 Python 版本 SDK: pip install --upgrade tencentcloud-sdk-python # 中国大陆地区的用户可以使用国内镜像源提高下载速度 pip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python。 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:6:1","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.2.2、上传本地录音文件识别 腾讯云提供的 API Explorer 可以很方便的生成代码和参数：https://console.cloud.tencent.com/api/explorer?Product=asr\u0026Version=2019-06-14\u0026Action=CreateRecTask\u0026SignVersion= 下面我提供一个自己的 demo ，运行前记得替换三个参数为真实值。其中 audio 是本地录音文件相对路径或绝对路径。 import json import base64 from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.asr.v20190614 import asr_client, models SecretId = \"xxxxxxxxxxxxxxxxxxxxxxxxx\" SecretKey = \"xxxxxxxxxxxxxxxxxxxxxxxxx\" audio = \"./audio.mp3\" # 将录音转为字符串 Data = \"\" with open(audio, 'rb') as f: # 读取音频数据 Data = f.read() Data = base64.b64encode(Data) Data = Data.decode() try: cred = credential.Credential(SecretId, SecretKey) httpProfile = HttpProfile() httpProfile.endpoint = \"asr.tencentcloudapi.com\" clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = asr_client.AsrClient(cred, \"\", clientProfile) req = models.CreateRecTaskRequest() params = { ''' 引擎模型类型。 这里的 k 指的是采样率 电话场景： • 8k_en：电话8k英语； • 8k_zh：电话8k中文普通话通用； 非电话场景： • 16k_zh：16k 中文普通话通用； • 16k_zh_video：16k 音视频领域； • 16k_en：16k 英语； • 16k_ca：16k 粤语； • 16k_ja：16k 日语； • 16k_zh_edu 中文教育； • 16k_en_edu 英文教育； • 16k_zh_medical 医疗； • 16k_th 泰语； ''' \"EngineModelType\": \"16k_zh\", ''' 识别声道数。注意：录音识别会自动将音频转码为填写的识别声道数 1：单声道； 2：双声道（仅支持 8k_zh 引擎模）。 ''' \"ChannelNum\": 1, ''' 识别结果返回形式。 0： 识别结果文本(含分段时间戳)； 1：词级别粒度的详细识别结果(不含标点，含语速值)； 2：词级别粒度的详细识别结果（包含标点、语速值） ''' \"ResTextFormat\": 0, ''' 语音数据来源。 0：语音 URL； 1：语音数据（post body）。 ''' \"SourceType\": 1, ''' 语音数据，当SourceType 值为1时必须填写，为0可不写。 要base64编码(采用python语言时注意读取文件应该为string而不是byte，以byte格式读取后要decode()。 编码后的数据不可带有回车换行符)。音频数据要小于5MB。 ''' \"Data\": Data } req.from_json_string(json.dumps(params)) resp = client.CreateRecTask(req) print(resp.to_json_string()) except TencentCloudSDKException as err: print(err) 输出 { \"Data\":{ \"TaskId\":1234567890 }, \"RequestId\":\"f1234567-89a4-1234-12d3-d56bdd9aac1a\" } 请求成功后，返回的 JSON 中 Data -\u003e TaskId 就是我们此次上传任务的 ID ，需要拿这个 ID 去轮训另一个接口，查询是否成功。 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:6:2","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.2.3、查询录音文件识别结果 腾讯云提供的 API Explorer 可以很方便的生成代码和参数：https://console.cloud.tencent.com/api/explorer?Product=asr\u0026Version=2019-06-14\u0026Action=DescribeTaskStatus\u0026SignVersion= 用 TaskID 查询识别结果。 import json from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.asr.v20190614 import asr_client, models SecretId = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" SecretKey = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" TaskId = 1234567890 try: cred = credential.Credential(SecretId, SecretKey) httpProfile = HttpProfile() httpProfile.endpoint = \"asr.tencentcloudapi.com\" clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = asr_client.AsrClient(cred, \"\", clientProfile) req = models.DescribeTaskStatusRequest() params = { \"TaskId\": TaskId } req.from_json_string(json.dumps(params)) resp = client.DescribeTaskStatus(req) print(resp.to_json_string()) except TencentCloudSDKException as err: print(err) 输出： { \"Data\":{ \"TaskId\":1234567890, \"Status\":2, \"StatusStr\":\"success\", \"Result\":\"[0:0.000,1:0.320] 识别结果。\\n[1:0.320,2:0.360] 识别结果。\\n[2:0.360,3:0.380] 识别结果。\\n[3:0.380,4:0.400] 识别结果。\\n[4:0.400,5:0.420] 识别结果。\\n\", \"ErrorMsg\":\"\", \"ResultDetail\":null }, \"RequestId\":\"12345678-1234-1234-1234-b11234567890\" } ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:6:3","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.3、一句话识别（V3） 文档地址：https://cloud.tencent.com/document/product/1093/35646 API Explorer：https://console.cloud.tencent.com/api/explorer?Product=asr\u0026Version=2019-06-14\u0026Action=SentenceRecognition\u0026SignVersion= ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:7:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.3.1、安装 Python SDK 在使用录音文件识别时，需要通过 pip 方式安装腾讯云提供的 Python 版本 SDK: pip install --upgrade tencentcloud-sdk-python # 中国大陆地区的用户可以使用国内镜像源提高下载速度 pip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python。 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:7:1","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾","Python"],"content":"2.3.2、源码 借助 API Explorer 生成的源码，我修改了一些东西： import json import base64 from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.asr.v20190614 import asr_client, models SecretId = \"xxxxxxxxxxxxxxxxxxxxxxxxxx\" SecretKey = \"xxxxxxxxxxxxxxxxxxxxxxxxxx\" audio = \"./一句话录音.mp3\" # 将录音转为字符串 Data = \"\" with open(audio, 'rb') as f: # 读取音频数据 Data = f.read() Data = base64.b64encode(Data) Data = Data.decode() try: cred = credential.Credential(SecretId, SecretKey) httpProfile = HttpProfile() httpProfile.endpoint = \"asr.tencentcloudapi.com\" clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = asr_client.AsrClient(cred, \"\", clientProfile) req = models.SentenceRecognitionRequest() params = { \"ProjectId\": 0, \"SubServiceType\": 2, \"EngSerViceType\": \"16k_zh\", \"SourceType\": 1, \"Data\": Data, \"VoiceFormat\": \"mp3\", \"UsrAudioKey\": \"uniqueKey-1\", } req.from_json_string(json.dumps(params)) resp = client.SentenceRecognition(req) print(resp.to_json_string()) except TencentCloudSDKException as err: print(err) 输出： { \"Result\":\"一句话录音识别内容\", \"AudioDuration\":59996, \"WordSize\":0, \"WordList\":null, \"RequestId\":\"12345678-4307-46ae-1234-beb3eb051234\" } REF 长时间的会议录音如何快速转化成文字：https://www.zhihu.com/question/21552953 如何用ffmpeg从mkv视频文件中提取音频？：https://www.zhihu.com/question/420452079 ","date":"2021-07-17","objectID":"/posts/about-extract-copywriting-from-audio-or-video.html:7:2","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/posts/about-extract-copywriting-from-audio-or-video.html"},{"categories":["折腾"],"content":"尽量简化不必要的流程，保持最简洁的操作，只需一条命令即可完成 hugo 静态 html 渲染、提交到 github 等一系列操作。 本操作只在 windows 平台下实践过，其他平台可自行尝试。 先安装 git bash：https://git-scm.com/downloads 确定hugo 博客的根目录：public 的上级目录就是你博客的根目录，我的根目录是 D:\\blog\\src D:\\BLOG\\SRC │ config.toml ├─archetypes ├─assets ├─content ├─data ├─layouts ├─public ├─resources ├─static └─themes 将 git 仓库（xxx.github.io）的 .git 文件夹和 CNAME 文件复制一份到博客根目录的 public 文件夹中，使 public 目录成为一个仓库。 配置 git alias 别名，这里我将 git blog 这个命令作为以后日常一键部署的命令，注意，此处的博客根目录不要填写错误。 git config --global alias.blog '!cd D:\\\\blog\\\\src;hugo;cd D:\\\\blog\\\\src\\\\public;git add .;git commit -m 'update';git push' 以后写好文章之后，就可以在任意目录执行这个命令： git blog 即可一键渲染、并提交推送到 github 上了，配合 git pages 使用更佳。 ","date":"2021-07-14","objectID":"/posts/hugo-one-click-deployment.html:0:0","series":null,"tags":["hugo"],"title":"hugo 一键部署","uri":"/posts/hugo-one-click-deployment.html"},{"categories":[],"content":"WizeTree 是一款 Windows 平台下的磁盘空间分析器。 通过可视化（图形化、树形化）的布局，你可以直观地看到在你硬盘上大的文件和文件夹。 内置文件管理器，支持查看文件树、文件夹大小排序、文件类型分析 wiztree 使用 NTFS 文件系统的 MFT 进行文件分析 (与著名的软件 everything 原理相同) 比 spacesniffer 的速度快数十倍，几秒钟就能完成全盘文件大小分析。 官网：https://www.diskanalyzer.com/ 预览图 ","date":"2021-07-14","objectID":"/posts/wiztree-the-fastest-disk-space-analyzer.html:0:0","series":null,"tags":["软件"],"title":"WizTree - 最快的磁盘空间分析器","uri":"/posts/wiztree-the-fastest-disk-space-analyzer.html"},{"categories":null,"content":"一名普通的 Java 后端攻城狮。 Technology stack PL：Java / Php / Python / C++ DB：MySQL / Redis OS：Linux / Windows Server Email jaded@foxmail.com 微信公众号 ","date":"2021-07-13","objectID":"/about.html:0:0","series":null,"tags":null,"title":"About","uri":"/about.html"},{"categories":null,"content":"CrownDaisy said, Less is more. xiaoz said, 生命不息，吾将折腾不止。 ","date":"2021-07-13","objectID":"/friends.html:0:0","series":null,"tags":null,"title":"友链 / 朋友们","uri":"/friends.html"},{"categories":["折腾"],"content":"有些网页上的视频是分成多个 ts 片段的，无法被 chrome的 各种嗅探器插件捕获，但通过 F12 开发工具监测网络（Network）时，在过滤器中输入 m3u8，可以发现一个独立的 m3u8 文件，这个文件就是记录了所有 ts 文件片段的一个播放列表。 文件内容大致像这样： 如果没有发现独立的 m3u8 文件，有可能每一个 ts 文件的地址中也是含有这个 m3u8 文件的名称的。把这个 m3u8 文件的完整地址截取出来。针对这种情况，我就不具体举例了，因为我还没遇到过。 到 header tab 里，复制一下这个 m3u8 文件的完整 url ，画红线的这部分就是。 假设这个地址是：https://xxx.abc.com/xxx/a.m3u8 可以使用ffmpeg（FFmpeg）命令下载合并输出为一个视频文件 ffmpeg -i https://xxx.abc.com/xxx/a.m3u8 -c copy output.mp4 ","date":"2021-07-13","objectID":"/posts/how-to-download-ts-streaming-video.html:0:0","series":null,"tags":["流媒体","FFmpeg"],"title":"如何下载 ts 流媒体视频","uri":"/posts/how-to-download-ts-streaming-video.html"},{"categories":["Python"],"content":"问题：有些批量下载的视频会带固定前缀，在视频播放器的播放列表里显示非常不友好。 场景：在 “D:\\纪录片\\中国通史” 路径下有 100 集视频文件，每个文件都带有固定前缀 “www.baidu.com 出品 微信公众号 xxx” 字样。 源码 RemoveFixedPrefix.py ，因为源码使用了 python fstring 的特性，需要在 python \u003e= 3.6 的版本中使用。 # coding=utf-8 import os # 目标路径 srcPath = \"D:\\纪录片\\中国通史\" fileList = os.listdir(srcPath) # 固定前缀 FixedPrefix = \"www.baidu.com出品 微信公众号xxx\" for fileName in fileList: oldFilePath = f\"{srcPath}/{fileName}\" # 跳过目录 if os.path.isdir(oldFilePath): continue newFileName = fileName.replace(FixedPrefix, \"\") newFilePath = f\"{srcPath}/{newFileName}\" try: os.rename(oldFilePath, newFilePath) except Exception as e: print(e) print(newFileName) ","date":"2021-06-21","objectID":"/posts/batch-file-fixed-prefix-removal-using-python.html:0:0","series":null,"tags":[],"title":"Python 批量去除文件固定前缀","uri":"/posts/batch-file-fixed-prefix-removal-using-python.html"},{"categories":["PHP"],"content":"结论推导 一、结论 # 最方便 echo strtotime('23:59:59') - time(); #最快 echo 86400 - (time() + 28800) % 86400; 二、推导过程 用 86400 减去今天已经过去了多少秒，即可求得今天还剩多少秒。 86400=24*3600，即一天的总秒数。 28800=8*3600，即 8 个小时的总秒数。 当前时间戳取模 86400 并不是今天已经过去了多少秒，因为时间戳起始时间并不是 0 点，而是 8 点整。所以，如果当前是早上 8 点整，取模 86400 后会等于 0，与我们的本意不符（求今天已经过去了多少秒）。 因此，要用当前时间戳加上 8 个小时的总秒数后再取模 86400，即可求得今天过去了多少秒。 结论是由下面的算法简化后得到的： 86400 - (time() + 8 * 3600) % 86400 三、REF https://segmentfault.com/a/1190000019844608 ","date":"2021-01-12","objectID":"/posts/how-to-use-php-to-calculate-the-number-of-seconds-left-in-the-day.html:0:0","series":null,"tags":[],"title":"PHP计算当天剩余秒数最方便和最快的方法","uri":"/posts/how-to-use-php-to-calculate-the-number-of-seconds-left-in-the-day.html"},{"categories":["Python"],"content":"此前写过一篇基于 BeautifulSoup 库开发的 demo，这次用 xpath 写。 源码 # -*- coding:utf-8 -*- # import requests from lxml import etree def get_headers(): headers = {} headers[\"content-type\"] = \"text/html;\" headers[ \"user-agent\"] = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36\" headers[\"host\"] = \"top.baidu.com\" return headers url = 'http://top.baidu.com/buzz?b=1' response = requests.get(url, headers=get_headers()) response.encoding = 'gbk' html = response.text if response.status_code != 200: print(f'返回状态码：{response.status_code}') exit(0) # 调用HTML类进行初始化 html = etree.HTML(html) # 提取该页面所有标题 result_all = html.xpath('//*[@id=\"main\"]/div[2]/div/table/tr/td[2]/a[1]') # 打印所有提取出的新闻标题 for v in result_all: print(v.text) 在开发时遇到一个有意思的坑，如果你测试时使用浏览器复制的热点标题的 title xpath，你会发现获取不到标题。你只需要把 tbody 标签去掉，就可以正常获取到标题了。 输出 31省新增本土病例85例:河北82例 佩洛西:众议院将第二次弹劾特朗普 河北新增49例本地无症状感染者 金正恩被推举为朝鲜劳动党总书记 北京新增1例确诊 4例无症状感染者 百度宣布组建智能汽车公司 美发生连环枪击案 一留学生身亡 日本发现新型变异新冠病毒 死刑!曾春亮案一审宣判 青藏高原云南等地降温雨雪来了 拼多多回应员工匿名发帖被辞退 石家庄新增确诊曾去过武汉汉正街 吉林新增4例本土无症状:2对夫妻 袁咏仪 送包给我是张智霖的福分 宋小女:这个结局也挺好 2020年CPI较上年上涨2.5% 全棉时代道歉疑似打广告 韩媒:韩军发现朝鲜举行阅兵式迹象 国会骚乱后特朗普没联系过彭斯 天津处罚过马路的低头族 被放生秃鹫赖警局每天伙食费150 比特币暴跌超10% 20岁小姐姐当汽车兵驰骋川藏线 北京乘出租车网约车需扫健康宝 网上买菜莫名被开通美团月付 自低风险区返乡要检测?多地出通知 车厘子价格腰斩 山东长岛海边现冰冻奇观似鸳鸯锅 武汉向石家庄捐赠50吨蔬菜 康辉说和21岁最大差别是脸的宽度 REF https://blog.csdn.net/qq_36523839/article/details/79992002 ","date":"2021-01-11","objectID":"/posts/python-get-baidu-live-hotspot.html:0:0","series":null,"tags":[],"title":"Python 爬取百度实时热点","uri":"/posts/python-get-baidu-live-hotspot.html"},{"categories":["Linux"],"content":"在高并发短连接的 TCP 服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量 socket 处于 TIME_WAIT 状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个 0~65535 的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。 在这个场景中，短连接表示 “业务处理 + 传输数据的时间 远远小于 TIMEWAIT 超时的时间” 的连接。Linux 默认的 TIME_WAIT 时长一般是 60 秒。 查看默认 timewait 时长 cat /proc/sys/net/ipv4/tcp_fin_timeout 查看连接状态统计 netstat -an | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 优化内核参数 vim /etc/sysctl.conf #追加内容 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 30 释义 开启 SYN Cookies。当出现 SYN 等待队列溢出时，启用 cookies 来处理，可防范少量 SYN 攻击，默认为 0，表示关闭。 net.ipv4.tcp_syncookies = 1 开启重用。允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，默认为 0，表示关闭。 net.ipv4.tcp_tw_reuse = 1 开启 TCP 连接中 TIME-WAIT sockets 的快速回收，默认为 0，表示关闭。 net.ipv4.tcp_tw_recycle = 1 修改系統默认的 TIMEOUT 时间（FIN_WAIT_2 状态的时长） net.ipv4.tcp_fin_timeout REF https://www.cnblogs.com/apanly/p/12431902.html https://zhuanlan.zhihu.com/p/79507132 ","date":"2021-01-03","objectID":"/posts/how-to-resolve-a-large-number-of-tcp-connections-with-time_wait-status.html:0:0","series":null,"tags":[],"title":"Linux 服务器出现大量 TIME_WAIT 状态的连接","uri":"/posts/how-to-resolve-a-large-number-of-tcp-connections-with-time_wait-status.html"},{"categories":["PHP"],"content":"无限级分类树生成可以使用递归或引用实现，但递归效率太慢，使用引用特性实现会是一个更好的方式。 源码 public function test() { // 初始数据 $items = array( array('id' =\u003e 1, 'pid' =\u003e 0, 'name' =\u003e '福建省'), array('id' =\u003e 2, 'pid' =\u003e 0, 'name' =\u003e '四川省'), array('id' =\u003e 3, 'pid' =\u003e 1, 'name' =\u003e '福州市'), array('id' =\u003e 4, 'pid' =\u003e 2, 'name' =\u003e '成都市'), array('id' =\u003e 5, 'pid' =\u003e 2, 'name' =\u003e '乐山市'), array('id' =\u003e 6, 'pid' =\u003e 4, 'name' =\u003e '成华区'), array('id' =\u003e 7, 'pid' =\u003e 4, 'name' =\u003e '龙泉驿区'), array('id' =\u003e 8, 'pid' =\u003e 6, 'name' =\u003e '崔家店路'), array('id' =\u003e 9, 'pid' =\u003e 7, 'name' =\u003e '龙都南路'), array('id' =\u003e 10, 'pid' =\u003e 8, 'name' =\u003e 'A店铺'), array('id' =\u003e 11, 'pid' =\u003e 9, 'name' =\u003e 'B店铺'), array('id' =\u003e 12, 'pid' =\u003e 8, 'name' =\u003e 'C店铺'), array('id' =\u003e 13, 'pid' =\u003e 1, 'name' =\u003e '泉州市'), array('id' =\u003e 14, 'pid' =\u003e 13, 'name' =\u003e '南安县'), array('id' =\u003e 15, 'pid' =\u003e 13, 'name' =\u003e '惠安县'), array('id' =\u003e 16, 'pid' =\u003e 14, 'name' =\u003e 'A镇'), array('id' =\u003e 17, 'pid' =\u003e 14, 'name' =\u003e 'B镇'), array('id' =\u003e 18, 'pid' =\u003e 16, 'name' =\u003e 'A村'), array('id' =\u003e 19, 'pid' =\u003e 16, 'name' =\u003e 'B村'), ); // 根据初始数据，生成一个以 id 为 key/下标 的数组，方便根据 pid 判断是否存在父级元素。 $items = array_column($items,null,'id'); //使用 php 的 \u0026 引用特性，遍历一次循环即可生成无限级分类树。（其他高级语言中也有类似的特性，诸如 C++ 的指针和 JAVA 的引用） $tree = []; foreach ($items as $item) { $id = $item['id']; $pid = $item['pid']; if (isset($items[$pid])) $items[$pid]['children'][] = \u0026$items[$id]; else $tree[] = \u0026$items[$id]; } $this-\u003esuccess('ok',$tree); } 输出 { \"code\": 1, \"msg\": \"ok\", \"data\": [ { \"id\": 1, \"pid\": 0, \"name\": \"福建省\", \"children\": [ { \"id\": 3, \"pid\": 1, \"name\": \"福州市\" }, { \"id\": 13, \"pid\": 1, \"name\": \"泉州市\", \"children\": [ { \"id\": 14, \"pid\": 13, \"name\": \"南安县\", \"children\": [ { \"id\": 16, \"pid\": 14, \"name\": \"A镇\", \"children\": [ { \"id\": 18, \"pid\": 16, \"name\": \"A村\" }, { \"id\": 19, \"pid\": 16, \"name\": \"B村\" } ] }, { \"id\": 17, \"pid\": 14, \"name\": \"B镇\" } ] }, { \"id\": 15, \"pid\": 13, \"name\": \"惠安县\" } ] } ] }, { \"id\": 2, \"pid\": 0, \"name\": \"四川省\", \"children\": [ { \"id\": 4, \"pid\": 2, \"name\": \"成都市\", \"children\": [ { \"id\": 6, \"pid\": 4, \"name\": \"成华区\", \"children\": [ { \"id\": 8, \"pid\": 6, \"name\": \"崔家店路\", \"children\": [ { \"id\": 10, \"pid\": 8, \"name\": \"A店铺\" }, { \"id\": 12, \"pid\": 8, \"name\": \"C店铺\" } ] } ] }, { \"id\": 7, \"pid\": 4, \"name\": \"龙泉驿区\", \"children\": [ { \"id\": 9, \"pid\": 7, \"name\": \"龙都南路\", \"children\": [ { \"id\": 11, \"pid\": 9, \"name\": \"B店铺\" } ] } ] } ] }, { \"id\": 5, \"pid\": 2, \"name\": \"乐山市\" } ] } ] ","date":"2020-12-16","objectID":"/posts/infinite-level-classification-tree-based-on-php-reference-feature.html:0:0","series":null,"tags":[],"title":"PHP基于引用特性实现的无限级分类树","uri":"/posts/infinite-level-classification-tree-based-on-php-reference-feature.html"},{"categories":["Linux"],"content":"Systemd 服务是一种以 .service 结尾的单元（unit）配置文件，用于控制由 Systemd 控制或监视的进程。简单说，用于后台以守护精灵（daemon）的形式运行程序。Systemd 广泛应用于新版本的 RHEL、SUSE Linux Enterprise、CentOS、Fedora 和 openSUSE 中，用于替代旧有的服务管理器 service。 一、如何创建一个服务？ 这里假设你已经自行编译安装好了 nginx，下面我们来创建一个 nginx.service 文件 vi /etc/systemd/system/nginx.service 内容如下： [Unit] Description=Nginx - high performance web server After=network.target [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop [Install] WantedBy=multi-user.target 重新加载服务配置文件，使创建的 nginx 服务生效： systemctl daemon-reload 这样我们就可以用 Systemd 的方式来管理 nginx 了，命令如下： #启动nginx systemctl start nginx #重载nginx systemctl reload nginx #停止nginx systemctl stop nginx #重启nginx systemctl restart nginx #如果需要开机启动 systemctl enable nginx #如果需要取消开机启动 systemctl disable nginx 二、关于 Systemd 服务 Systemd 服务的内容主要分为三个部分，控制单元（unit）的定义、服务（service）的定义、以及安装（install）的定义。 ","date":"2020-12-01","objectID":"/posts/how-to-create-a-linux-systemd-service.html:0:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/posts/how-to-create-a-linux-systemd-service.html"},{"categories":["Linux"],"content":"2.1、控制单元 unit 从上面的例子中我们看到 Unit 内容如下： [Unit] Description=Nginx - high performance web server After=network.target Description：代表整个单元的描述，可根据需要任意填写。 Before/After：指定启动顺序。 network.target 代表有网路，network-online.target 代表一个连通着的网络。 ","date":"2020-12-01","objectID":"/posts/how-to-create-a-linux-systemd-service.html:1:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/posts/how-to-create-a-linux-systemd-service.html"},{"categories":["Linux"],"content":"2.2、服务本体 service [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop Type: 服务的类型，各种类型的区别如下所示 simple：默认，这是最简单的服务类型。意思就是说启动的程序就是主体程序，这个程序要是退出那么一切皆休。 forking：标准 Unix Daemon 使用的启动方式。启动程序后会调用 fork () 函数，把必要的通信频道都设置好之后父进程退出，留下守护精灵的子进程。 oneshot：适用于那些被一次性执行的任务或者命令，它运行完成后便了无痕迹。因为这类服务运行完就没有任何痕迹，我们经常会需要使用 RemainAfterExit=yes。意思是说，即使没有进程存在，Systemd 也认为该服务启动成功了。同时只有这种类型支持多条命令，命令之间用；分割，如需换行可以用 \\。 dbus：这个程序启动时需要获取一块 DBus 空间，所以需要和 BusName= 一起用。只有它成功获得了 DBus 空间，依赖它的程序才会被启动。 ExecStart：在输入的命令是 start 时候执行的命令，这里的命令启动的程序必须使用绝对路径，比如你必须用 /sbin/arp 而不能简单的以环境变量直接使用 arp。 ExecStop：在输入的命令是 stop 时候执行的命令，要求同上。 ExecReload：这个不是必需，如果不写则你的 service 就不支持 restart 命令。ExecStart 和 ExecStop 是必须要有的。 ","date":"2020-12-01","objectID":"/posts/how-to-create-a-linux-systemd-service.html:2:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/posts/how-to-create-a-linux-systemd-service.html"},{"categories":["Linux"],"content":"2.3、安装部分 install [Install] WantedBy=multi-user.target WantedBy：运行级别 / 设置服务被谁装载，一般设置为 multi-user.target（从 Centos7 以后 bai 采用 target 概念来定义运行级别，multi-user.target 是第三级别） ","date":"2020-12-01","objectID":"/posts/how-to-create-a-linux-systemd-service.html:3:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/posts/how-to-create-a-linux-systemd-service.html"},{"categories":["Linux"],"content":"2.4、存放的位置 Systemd Service 位于 /etc/systemd/system（供系统管理员和用户使用），/usr/lib/systemd/system（供发行版打包者使用），我们一般使用前者即可。 3、总结 Systemd Service 是一种替代 /etc/init.d/ 下脚本的更好方式，它可以灵活的控制你什么时候要启动服务，一般情况下也不会造成系统无法启动进入紧急模式。所以如果想设置一些开机启动的东西，可以试着写 Systemd Service。当然了，前提是你使用的 Linux 发行版是支持它的才行。 REF https://www.xiaoz.me/archives/14458 https://segmentfault.com/a/1190000014740871 https://zh.opensuse.org/openSUSE:How_to_write_a_systemd_service ","date":"2020-12-01","objectID":"/posts/how-to-create-a-linux-systemd-service.html:4:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/posts/how-to-create-a-linux-systemd-service.html"},{"categories":["Linux"],"content":"连接国外或内网 centos7 主机时发现会因为 DNS 的问题造成 SSH 连接速度慢。 SSH 登录太慢可能是 DNS 解析的问题，默认配置下 sshd 初次接受 ssh 客户端连接的时候会自动反向解析客户端 IP 以得到 ssh 客户端的域名或主机名。如果这个时候 DNS 的反向解析不正确，sshd 就会等到 DNS 解析超时后才提供 ssh 连接，这样就造成连接时间过长、ssh 客户端等待的情况，一般为 10-30 秒左右。有个简单的解决办法就是在 sshd 的配置文件（sshd_config）里取消 sshd 的反向 DNS 解析。 编辑 ssh 配置文件 vi /etc/ssh/sshd_config 找到 UseDNS 设置为 no 重启 ssh 服务即可 systemctl restart sshd ","date":"2020-11-28","objectID":"/posts/solution-for-ssh-login-too-slow.html:0:0","series":null,"tags":["ssh"],"title":"CentOS SSH 登录太慢的解决方法","uri":"/posts/solution-for-ssh-login-too-slow.html"},{"categories":["Python"],"content":"读取本地文件内容，对文本内容进行中文分词，统计词频后，生成词云图。 一、生成矩形颜色随机的词云图 读取本地文件内容，对文本内容进行中文分词，统计词频后，生成矩形随机颜色的词云图。 import logging import collections import re import math import jieba from wordcloud import WordCloud jieba.setLogLevel(logging.INFO) # 创建停用词列表 def stopwordslist(): # 按行读入 stopwords = [line.strip() for line in open('chinsesstop.txt', encoding='UTF-8').readlines()] # 分割为单个字符（列表解析） stopwords = [k for s in stopwords for k in s] return stopwords # # 指定字符串方式 # text = \"collections在python官方文档中的解释是High-performance container datatypes，直接的中文翻译解释高性能容量数据类型。它总共包含五种数据类型\" # 文件读取方式 f = open(\"./article.txt\", \"r\", encoding=\"utf-8\") text = f.read() f.close() # 生成词云的词频限制，选取前30% TopWordFrequencyPercentage = 30 TopWordFrequencyPercentage /= 100 # 词云图片生成路径（当前目录下的 wordcloud_rectangle.png 文件） PicSavePath = \"./wordcloud_rectangle.png\" # jieba分词 seg = jieba.cut(text) seg = \" \".join(seg) seg = seg.strip() # 去除标点符号 seg = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:：。？、~@#￥%……\u0026*（）]+\", \" \", seg) # 只取中文 # seg = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', seg) # 转换为list seg = seg.split(\" \") # 过滤空字符和None seg = list(filter(None, seg)) # print(seg) # 创建一个停用词列表 stopwords = stopwordslist() # print(stopwords) # 过滤停用词（列表解析） seg = [v for v in seg if v not in stopwords] # print(seg) # 统计词频 word_counts = collections.Counter(seg) # print(word_counts) # print(math.ceil(len(word_counts) * 0.3)) # # 获取词频降序排列的前30% # word_counts_top_50_percent = word_counts.most_common(math.ceil(len(word_counts) * 0.3)) # print(word_counts_top_50_percent) # 生成词云 wc = WordCloud( # 限制词数（根据词频限制，计算个数，向上取整） max_words=math.ceil(len(word_counts) * TopWordFrequencyPercentage), # 设置背景宽 width=500, # 设置背景高 height=350, # 最大字体 max_font_size=50, # 最小字体 min_font_size=10, # 设置字体文件路径，不指定就会出现乱码。 font_path='./MSYH.TTC', # 设置背景色 background_color='white', ) # 根据词频产生词云 wc.generate_from_frequencies(word_counts) # 生成词云图片文件 wc.to_file(PicSavePath) 效果图： 二、根据图片生成规定形状的颜色相近的词云图 读取本地文件内容，对文本内容进行中文分词，统计词频后，根据背景图片生成规定形状和颜色的词云图。 import logging import collections import re import math from random import randint import jieba from wordcloud import WordCloud from wordcloud import ImageColorGenerator from PIL import Image import numpy as np jieba.setLogLevel(logging.INFO) # 创建停用词列表 def stopwordslist(): # 按行读入 stopwords = [line.strip() for line in open('chinsesstop.txt', encoding='UTF-8').readlines()] # 分割为单个字符（列表解析） stopwords = [k for s in stopwords for k in s] return stopwords # 自定义颜色函数（在绘制词云图时发现有的字颜色为黄色导致看不清因此需要修改整个词云图的色调为冷色调 蓝绿色） def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None): # what is HSL? https://baike.baidu.com/item/HSL/1443144?fr=aladdin H = randint(120, 250) S = int(100.0 * 255.0 / 255.0) L = int(100.0 * float(randint(60, 120)) / 255.0) return \"hsl({}, {}%, {}%)\".format(H, S, L) # # 指定字符串方式 # text = \"collections在python官方文档中的解释是High-performance container datatypes，直接的中文翻译解释高性能容量数据类型。它总共包含五种数据类型\" # 文件读取方式 f = open(\"./article.txt\", \"r\", encoding=\"utf-8\") text = f.read() f.close() # 生成词云的词频限制，选取前30% TopWordFrequencyPercentage = 30 TopWordFrequencyPercentage /= 100 # 词云图片生成路径（当前目录下的 wordcloud_arbitrary_shape.png 文件） PicSavePath = \"./wordcloud_arbitrary_shape.png\" # jieba分词 seg = jieba.cut(text) seg = \" \".join(seg) seg = seg.strip() # 去除标点符号 seg = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:：。？、~@#￥%……\u0026*（）]+\", \" \", seg) # 只取中文 # seg = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', seg) # 转换为list seg = seg.split(\" \") # 过滤空字符和None seg = list(filter(None, seg)) # print(seg) # 创建一个停用词列表 stopwords = stopwordslist() # print(stopwords) # 过滤停用词（列表解析） seg = [v for v in seg if v not in stopwords] # print(seg) # 统计词频 word_counts = collections.Counter(seg) # print(word_counts) # print(math.ceil(len(word_counts) * 0.3)) # # 获取词频降序排列的前30% # word_counts_top_50_percent = word_counts.most_common(math.ceil(len(word_counts) * 0.3)) # print(word_counts_top_50_percent) # 词云形状 mask = np.array(Image.open(\"./background1.png\")) # 根据图片颜色设置词云颜色(如果背景图片是纯色背景会报 NotImplementedError: Gray-scale images TODO 错","date":"2020-11-18","objectID":"/posts/split-words-generate-word-clouds-based-on-word-frequency-and-background-images.html:0:0","series":null,"tags":[],"title":"Python 中文分词，根据词频和背景图片生成词云","uri":"/posts/split-words-generate-word-clouds-based-on-word-frequency-and-background-images.html"},{"categories":["Python"],"content":"有时 pip 不指定源安装会比较慢，甚至会安装失败。 #阿里源 pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ #豆瓣 pip install -r requirements.txt -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com #清华大学 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/ ","date":"2020-11-17","objectID":"/posts/pip-source-specific-installation.html:0:0","series":null,"tags":[],"title":"Python pip 指定源安装","uri":"/posts/pip-source-specific-installation.html"},{"categories":["Python"],"content":"在查看别人的 Python 项目时，经常会看到一个 requirements.txt 文件，里面记录了当前程序的所有依赖包及其精确版本号，与 npm 的 package.json 很像。其作用是用来在另一台 PC 上重新构建项目所需要的运行环境依赖。 # 导出依赖 pip freeze \u003e requirements.txt # 安装依赖 pip install -r requirements.txt ","date":"2020-11-17","objectID":"/posts/python-export-or-install-requirements.txt-dependencies.html:0:0","series":null,"tags":[],"title":"Python 导出 or 安装 requirements.txt 依赖","uri":"/posts/python-export-or-install-requirements.txt-dependencies.html"},{"categories":["Python"],"content":"安装完 anaconda 后，打开 Anaconda Powershell Prompt (anaconda3) 执行命令。 一、Anaconda # 查看 conda 版本号 conda --version # 查看系统当前已有的 Python 环境 conda info --envs # 添加一个名为 python27，Python 版本为 2.7 的环境 conda create --name python27 python=2.7 # 查看当前环境的 Python 版本 python --version # 切换 Python 环境到刚才新添加的 Python2.7 conda activate python27 # 切回原来的 Python 环境 conda deactivate python27 # 或 conda activate base # 删除 python27 这个环境 conda remove --name python27 --all # 添加清华源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes # 查看镜像列表 conda config --show channels # 删除某个镜像 conda config --remove channels #如：https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ # 包管理 conda install \u003c包名\u003e 安装指定包 conda remove \u003c包名\u003e 移除指定包 conda update \u003c包名\u003e 更新指定包 二、PyCharm ","date":"2020-11-13","objectID":"/posts/pycharm-using-anaconda-to-manage-multi-version-python-environments.html:0:0","series":null,"tags":[],"title":"PyCharm 使用 Anaconda 管理多版本 Python 环境","uri":"/posts/pycharm-using-anaconda-to-manage-multi-version-python-environments.html"},{"categories":["Python"],"content":"先说结论：推荐使用 openyxl 库，而不是 xlwt 库。使用 xlwt 库创建并向 .xls 文件写入内容时，其单个 sheet 限制最大行数为 65535。而使用 openpyxl 库创建并向 .xlsx 文件写入内容时，其单个 sheet 限制最大行数为 1048576，最大列数为 16384。 一、openpyxl 库 openyxl 库文档地址：https://openpyxl.readthedocs.io/en/stable/usage.html / https://pypi.org/project/openpyxl import openpyxl ExeclFileSavePath = \"./test.xlsx\" wb = openpyxl.Workbook() rowInit = 1 rowLimit = 50 columnInit = 1 columnLimit = 5 # 默认Sheet ws = wb.active for row in range(rowInit, rowLimit + 1): for column in range(columnInit, columnLimit + 1): ws.cell(row, column).value = row # 创建一个sheet Pi wsA = wb.create_sheet(title=\"Pi\") for row in range(rowInit, rowLimit): wsA.append(range(columnInit, columnLimit)) # 保存文件 wb.save(ExeclFileSavePath) 二、xlwt 库 xlwt 库文档地址：https://xlwt.readthedocs.io/en/latest / https://pypi.org/project/xlwt import xlwt ExeclFileSavePath = \"./test.xls\" workbook = xlwt.Workbook(encoding='utf-8') sheet1 = workbook.add_sheet('sheet1', cell_overwrite_ok=True) # 给excel文件添加sheet rowInit = 1 rowLimit = 50 columnInit = 1 columnLimit = 5 for row in range(rowInit - 1, rowLimit): for column in range(columnInit - 1, columnLimit): temp = row # 单元格内容 sheet1.write(row, column, temp) # sheet1.write_merge(0, 3, 1, 1, '合并') # 合并单元格 workbook.save(ExeclFileSavePath) ","date":"2020-11-11","objectID":"/posts/create-an-excel-file-and-write-to-it-using-python.html:0:0","series":null,"tags":[],"title":"Python 如何创建一个 Excel 文件并向其写入内容？","uri":"/posts/create-an-excel-file-and-write-to-it-using-python.html"},{"categories":["MySQL"],"content":"隐藏在各项配置文件之后的，是 MySQL 的思想和设计方案。 查看当前所有配置 showvariables;一、表相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:0:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"表数据独立存储 表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的，从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。 我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。 因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop/truncate table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 innodb_file_per_table = ON 二、日志相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:1:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"bin log 格式 如果是使用 delete 语句误删了数据行，可以用 Flashback 工具通过闪回把数据恢复回来。Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保以下两个配置。 binlog_format=row binlog_row_image=FULL ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:2:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"bin log 文件体积限制 / 自动过期 row 格式记录的数据更完整，在数据恢复方面有极大优势，但缺点是生成的日志量大。因此需要根据业务需求和服务器配置对 binlog 设置单个文件最大体积限制和保留时长的限制。 expire_logs_days = 30 max_binlog_size = 512M ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:3:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"双 “1” 配置 一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 redo log 用于保证 crash-safe 能力，innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后 redo log 数据不丢失。 sync_binlog = 1 innodb_flush_log_at_trx_commit = 1 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:4:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"redo log 文件体积 / 个数 利用 WAL 技术，MySQL 将随机写转换成了顺序写，大大提升了数据库的性能。但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些，造成性能的间歇性下降。 “redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。 一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。 这两个属性是只读的，需要通过修改配置文件并重启 MySQL 生效。 innodb_log_file_size=512M innodb_log_files_in_group=2 三、buffer 相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:5:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"缓冲池大小 缓冲池内存大小不够，意味着可用的数据页少，脏页比例容易接近 75%，造成性能间歇性抖动。 innodb_buffer_pool_size=1G ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:6:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"join 联表查询缓冲池大小 join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。如果 join 语句较慢，有可能是因为缓冲池过小。 join_buffer_size=32M 四、硬盘 IO 相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:7:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"IO 读写能力 如果没能合理地设置 innodb_io_capacity 参数，会导致一些性能问题。比如 MySQL 的写入速度很慢，TPS 很低，但是数据库主机的 IO 压力并不大。 如果主机磁盘用的是 SSD，但是 innodb_io_capacity 的值设置的是 300。于是，InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。 测试完成后可以得到服务器硬盘的 IOPS，根据这个值来设置：（200 是默认值） innodb_io_capacity = 200 centos 安装 fio，测试硬盘 io yum install -y libaio-devel fio cd /;mkdir test;cd /test;touch file;fio -filename=/test/file -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=4k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest windows 可以使用 AS SSD 软件测试 IOPS ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:8:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"脏页刷写连坐机制 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个 “邻居” 也带着一起刷掉；而且这个把 “邻居” 拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的 “连坐” 机制，值为 0 时表示不找邻居，自己刷自己的。 找 “邻居” 这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。 而如果使用的是 SSD 这类 IOPS 比较高的设备的话，建议把 innodb_flush_neighbors 的值设置成 0： 因为这时候 IOPS 往往不是瓶颈，而 “只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。 innodb_flush_neighbors = 0 五、线程相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:9:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"并发线程上限数 通常情况下，我们建议把 innodb_thread_concurrency 设置为 64~128 之间的值。并发连接和并发查询并不是同一个概念，你在 show processlist 的结果里，看到的几千个连接，指的就是并发连接。而 “当前正在执行” 的语句，才是我们所说的并发查询。 并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是 CPU 杀手。这也是为什么我们需要设置 innodb_thread_concurrency 参数的原因。 在线程进入锁等待以后，并发线程的计数会减一，也就是说等行锁（也包括间隙锁）的线程是不算在 128 里面的。MySQL 这样设计是非常有意义的。因为，进入锁等待的线程已经不吃 CPU 了；更重要的是，必须这么设计，才能避免整个系统锁死。 innodb_thread_concurrency=128 六、锁相关 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:10:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"自增锁模式 MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。 这个参数的值被设置为 0 时，表示采用之前 MySQL 5.0 版本的策略，即语句执行结束后才释放锁； 这个参数的值被设置为 1 时：普通 insert 语句，自增锁在申请之后就马上释放；类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放； 这个参数的值被设置为 2 时，所有的申请自增主键的动作都是申请后就释放锁。 在生产上，尤其是有 insert … select 这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置： innodb_autoinc_lock_mode=2 ，并且 binlog_format=row. 这样做，既能提升并发性，又不会出现数据一致性问题。 innodb_autoinc_lock_mode=2 七、SQL 部分 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:11:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"安全模式 为了防止忘记在 delete 或者 update 语句中写 where 条件、where 条件里面没有包含索引字段、没有加入 limit 限制引起的非安全 SQL 语句造成数据误删等，写入该配置后，这些非安全语句执行就会报错。 sql_safe_updates = ON 配置模板 innodb_file_per_table = ON binlog_format=row binlog_row_image=FULL expire_logs_days = 30 max_binlog_size = 512M sync_binlog = 1 innodb_flush_log_at_trx_commit = 1 innodb_log_file_size=512M innodb_log_files_in_group=2 innodb_buffer_pool_size=1G join_buffer_size=32M innodb_io_capacity = 300 innodb_flush_neighbors = 0 innodb_thread_concurrency=128 innodb_autoinc_lock_mode=2 ","date":"2020-11-07","objectID":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html:12:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/posts/mysql-5.7-recommended-parameter-configuration-for-production-environments.html"},{"categories":["MySQL"],"content":"在日常开发中遇到的一些问题。 一、查看数据库的各个表占用的文件大小 以查看 test 数据库为例： SELECTtable_schemaAS'数据库',table_nameAS'表名',engineAS'存储引擎',table_commentAS'备注',table_rowsAS'记录数',TRUNCATE(data_length/1024/1024,2)AS'数据大小(MB)',TRUNCATE(index_length/1024/1024,2)AS'索引大小(MB)'FROMinformation_schema.TABLESWHEREtable_schema='test'ORDERBYtable_rowsDESC;二、查找持续时间超过 60s 的事务 select*frominformation_schema.innodb_trxwhereTIME_TO_SEC(timediff(now(),trx_started))\u003e60三、查看当前线程处理情况 配套 kill 语句可以处理突发事件 showfullprocesslist;killId;四、优化表 optimizetable`table_name_A`;重新组织表数据和相关索引数据的物理存储，以减少存储空间，提高访问表时的 I/O 效率。 但此操作会锁表，需要避开业务高峰期。 五、重建表 altertableAengine=InnoDB;试想一下，如果你现在有一个表 A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。 显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。 而在 MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 我给你简单描述一下引入了 Online DDL 之后，重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件； 用临时文件替换表 A 的数据文件。 可以看到，不同之处在于，由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。 DDL 之前是要拿 MDL 写锁的，这样还能叫 Online DDL 吗？ alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。 需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用 GitHub 开源的 gh-ost 来做。 另外，使用 alter table t engine=InnoDB 有可能会让一个表占用的空间反而变大： 1、就是这个表本身就已经没有空洞，比如说刚刚做过一次重建表操作； 2、在 DDL 期间，如果刚好有外部的 DML 在执行，这期间可能会引入一些新的空洞； 3、在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不是 “最” 紧凑的。 ","date":"2020-11-05","objectID":"/posts/common-maintenance-sql-for-mysql.html:0:0","series":null,"tags":[],"title":"MySQL 的常用维护语句","uri":"/posts/common-maintenance-sql-for-mysql.html"},{"categories":["MySQL"],"content":"MySQL 的四种事务隔离级别 / 如何切换 MySQL 的默认全局事务隔离级别 / 了解 session 和 global 关键字 一、查看当前 MySQL 版本号 selectversion();二、查看当前全局事务隔离级别 ","date":"2020-11-04","objectID":"/posts/modifying-the-mysql-default-transaction-isolation-level.html:0:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/posts/modifying-the-mysql-default-transaction-isolation-level.html"},{"categories":["MySQL"],"content":"2.1、MySQL5.6 及其更早的版本 select@@global.tx_isolation;","date":"2020-11-04","objectID":"/posts/modifying-the-mysql-default-transaction-isolation-level.html:1:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/posts/modifying-the-mysql-default-transaction-isolation-level.html"},{"categories":["MySQL"],"content":"2.2、MySQL5.7 及更高版本 select@@global.transaction_isolation;1、MySQL5.7 引入了 transaction_isolation 用来代替 tx_isolation，并在 MySQL8.0.3 去掉了 tx_isolation，在 MySQL5.7 及更高版本中建议使用 transaction_isolation 2、若要查看当前会话的事务隔离级别，可以去掉 global. 使用 SELECT @@transaction_isolation。同理，若只想针对当前 session 设置事务隔离级别，可将 global 关键字替换为 session 三、MySQL 的四个事务隔离级别 | 事务隔离级别 | 脏读 | 不可重复读 | 幻读 | | —- | —- | | 读未提交（read-uncommitted） | 是 | 是 | 是 | | 读提交（read-committed） | 否 | 是 | 是 | | 可重复读（repeatable-read） | 否 | 否 | 是 | | 串行化（serializable） | 否 | 否 | 否 | 四、修改 MySQL 全局默认事务隔离级别 ","date":"2020-11-04","objectID":"/posts/modifying-the-mysql-default-transaction-isolation-level.html:2:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/posts/modifying-the-mysql-default-transaction-isolation-level.html"},{"categories":["MySQL"],"content":"4.1、MySQL5.6 及其更早的版本 setglobaltx_isolation='read-uncommitted';setglobaltx_isolation='read-committed';setglobaltx_isolation='repeatable-read';setglobaltx_isolation='serializable';","date":"2020-11-04","objectID":"/posts/modifying-the-mysql-default-transaction-isolation-level.html:3:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/posts/modifying-the-mysql-default-transaction-isolation-level.html"},{"categories":["MySQL"],"content":"4.2、MySQL5.7 及更高版本 setglobaltransaction_isolation='read-uncommitted';setglobaltransaction_isolation='read-committed';setglobaltransaction_isolation='repeatable-read';setglobaltransaction_isolation='serializable';","date":"2020-11-04","objectID":"/posts/modifying-the-mysql-default-transaction-isolation-level.html:4:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/posts/modifying-the-mysql-default-transaction-isolation-level.html"},{"categories":["Nginx"],"content":"我们知道，在 nginx 的 if 中是不能写 limit_req 和 limit_conn 的。也就是说在 nginx 的配置文件中，我们无法通过 if 对请求参数做逻辑判断，从而实现对复杂请求参数的精准限流。 应用场景：针对某一个流量特别大的入口的某一个特定 GET 请求参数做限流，比如来自微信小程序的 API 请求入口（https://example.com/app/index.php?i=99\u0026v=9.9.9\u0026m=xxxx\u0026from=wxapp），我希望 from = wxapp 时进行限流，若 from 不等于 wxapp 则不进行限流。 二、使用 map 针对请求参数限流 在 nginx 的 http 部分写入： limit_req_zone $paramFrom zone=from:10m rate=30r/s; map $arg_from $paramFrom { wxapp wxapp; } 这里的 rate 我设为了每秒处理 30 个请求，如果 limit_req 没有设置 burst，则默认为 0。 rate 支持 r/s 和 r/m 两种模式。 在 server 部分写入： location ^~ /app/index.php { limit_req zone=from; #limit_rate 512k; #宝塔PHP文件处理 try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi-56.sock; fastcgi_index index.php; include fastcgi.conf; include pathinfo.conf; } 三、关于 Nginx 的 map 模块 使用 map 模块，可以根据一个或多个变量组合成一个新的变量。通过判断新的变量，我们可以处理非常复杂的业务逻辑。本文是以 map 模块用于限流为示例，简单展示一下 map 的使用方法。 四、原理 http_limit_req_module 和 http_limit_conn_module 两个模块是在 nginx 的 preaccess 阶段对请求做拦截，也就是说使用 limit_req_zone 或是 limit_conn_zone 对请求都是可以的。 但两者有本质上的区别，request 和 connection 是不同的，在实际应用中应注意区别。 connection 是连接，即常说的 tcp 连接，通过三次握手而建立的。 request 是指请求，即 http 请求，（注意，tcp 连接是有状态的，而构建在 tcp 之上的 http 却是无状态的协议）。 limit_req_zone 或是 limit_conn_zone 对请求的限制有效性取决于 key 的设计，通常使用 realip 模块获取到的客户端 IP。但 IP 是针对全体用户做的限流，显然不能满足我们的需求。所以我们需要使用 map 模块来匹配特定的请求参数生成一个变量，将这个变量作为 limit_req_zone 或是 limit_conn_zone 的 key，这样就可以实现我们的需求了。 ","date":"2020-09-17","objectID":"/posts/nginx-limits-flow-based-on-specific-request-parameters.html:0:0","series":null,"tags":["限流"],"title":"Nginx 根据特定请求参数做限流","uri":"/posts/nginx-limits-flow-based-on-specific-request-parameters.html"},{"categories":["Nginx"],"content":"使用 nginx 的 http_limit_conn_module 模块可以在 nginx 的 preaccess 阶段对请求的并发做拦截。限制的有效性取决于 key 的设计，通常使用 realip 模块获取到的客户端 IP。 应用场景：假如你只希望限流用户的访问入口，但不希望管理后台也被纳入限流的范围内，因为在操作管理后台时，用户访问量激增，nginx 会频繁返回 503，那么管理后台将处于不可操作的状态。 一、编写 location 规则 假设我需要对 /app/index.php 这个路径做并发量的控制（例如：https://example.com/app/index.php?i=99\u0026t=0\u0026v=5.5.5\u0026from=wxap），首先要确定的事情就是编写的 location 规则是否能被正确匹配到。下面是 nginx location 的写法： location ^~ /app/index.php { return 404; } 加上这条 location 规则后，如果访问（https://example.com/app/index.php?i=99\u0026t=0\u0026v=5.5.5\u0026from=wxap）这样的链接，nginx 会直接返回 404 错误。那么说明这段 location 是能够匹配到的。 二、限制并发量 在 nginx 的 http 部分添加： limit_conn_zone $binary_remote_addr zone=perip:10m; limit_conn_zone $server_name zone=perserver:10m; $binary_remote_addr 是二进制格式的 IPv4 地址，这个 IP 是请求者的 IP。将远程客户端的 IP 地址作为 zone 的 key，目的是为了对单个客户端做并发限制。 $server_name 指的是当前站点的名称，将这个作为 zone 的 key，目的是为了对某个站点做总体的并发限制。 稍稍修改一下上面的 location 规则，设置为站点并发量为 300，单个 IP 并发量限制 25： location ^~ /app/index.php { limit_conn perserver 300; limit_conn perip 25; limit_rate 128k; #宝塔面板默认的 enable-php-56.conf 规则，对php文件的响应处理 try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi-56.sock; fastcgi_index index.php; include fastcgi.conf; include pathinfo.conf; } limit_conn 的作用是，在某个 zone 下的并发限制为多少。 limit_rate 限制的是 nginx 向客户端传送响应的速率。 limit_conn 和 limit_req 不能设置在 if 指令中，所以如果针对不同的 URL 进行限流，只能通过不同的 location 实现。 limit_rate 可以在 if 指令中，可以使用 if 指令匹配 URL 实现不同 URL 的限流。 使用 jmeter 等相关并发测试工具可以测到，确实对 /app/index.php/ 请求做了并发量的限制，超出并发量的部分 nginx 会默认返回 503。 ","date":"2020-09-16","objectID":"/posts/nginx-limits-flow-to-specific-paths-or-entry-files.html:0:0","series":null,"tags":["限流"],"title":"Nginx 针对特定路径或入口文件限流","uri":"/posts/nginx-limits-flow-to-specific-paths-or-entry-files.html"},{"categories":["PHP","Redis"],"content":"ThinkPHP 如何使用 Redis 实现悲观锁解决高并发情况下读写带来的脏读问题 / ThinkPHP5.1 / Redis Cache / File Cache 测试。 在用户量 / 客户端数量比较少的时候，只要系统的业务逻辑是正确的，一般都不会发现有什么问题。但随着用户量 / 客户端数量逐渐增多，高并发带来的问题就会逐渐出现，而脏读是众多问题的其中之一。 一、无并发控制，会带来什么问题？ 本文以 ThinkPHP5.1.39 的代码作为案例，下面是一个 File Cache 读写操作： public function fileCacheCase(){ $keyName = \"test\"; $keyValue = 996; //写入缓存 Cache::set($keyName, $keyValue, 3600); //从缓存中获取值 $data = Cache::get($keyName); //删除缓存 Cache::rm($keyName); echo \"OK! $data\"; } 访问这个 function，会输出 OK! 996 。无论你访问几次，结果都是如此，但仅限于单线程的情况（只有你自己一个人在访问这个 function），如果是多个人同时不停的访问这个 function，还会是这样吗？想一想 😛 使用 jmeter 测试一下，120 线程测试了十几秒，发现了 3 种不同的返回结果。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:0:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"1.1、返回了 OK! 996 与单线程时的结果一致，是正常处理逻辑。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:1:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"1.2、只返回了 OK！而不是 OK! 996 说明缓存不存在，原因是：在 A 线程将 996 写入缓存后，B 线程将缓存删除了。此时 A 线程从缓存中读出来的数据为 null，所以 A 线程输出了 OK! ，而不是 OK! 996。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:2:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"1.3、返回了一个 500 错误 报错的内容是： file_get_contents（…）No such file or directory。 显然是 cache 文件夹下的某个缓存文件不存在，所以引起了这个错误。原因是：A 线程在删除缓存后，B 线程也在执行删除缓存的操作。当缓存文件已被删除时，再执行删除缓存文件的操作，自然就报了文件不存在的错误。（实测 120 个线程并发，总计 500 个请求，异常率 0.20%） 尽管我修改了 File Cache 的 133 行，在删除前判断文件是否存在，虽然异常率降低了，但依然无法从根本上解决问题。可以看到的是，在高并发场景下，问题已经显现出来了。 下面我们用 redis 缓存试试看： public function fileCacheCase(){ $keyName = \"test\"; $keyValue = 996; //写入缓存 Cache::store('redis')-\u003eset($keyName, $keyValue, 3600); //从缓存中获取值 $data = Cache::store('redis')-\u003eget($keyName); //删除缓存 Cache::store('redis')-\u003erm($keyName); echo \"OK! $data\"; } 经过测试，与上面的 3 种情况一致。（根据 thinkphp5.1 的官方文档，我使用的是 store 来切换到 redis，但不知道为何，仍然会报 File Cache 驱动的 No such file or directory/unlink 错误，十分诡异）。 如何解决高并发场景下带来的脏读问题？ 答案是：使用锁机制。 二、关于锁机制 根据锁的控制范围，可分为单机锁 / 分布式锁 2 种。根据锁的实现思想，可分为悲观锁 / 乐观锁 2 种。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:3:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"2.1、单机锁 即为单机环境的锁，无分布式设计。 常用的实现工具： Redis Memcached ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:4:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"2.2、分布式锁 为了防止分布式系统中的多个进程之间相互干扰，我们需要一种分布式协调技术来对这些进程进行调度。而这个分布式协调技术的核心就是来实现这个分布式锁。 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行 高可用的获取锁与释放锁 高性能的获取锁与释放锁 具备锁失效机制，防止死锁 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败 常用的实现工具： Zookeeper Redis Memcached Chubby ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:5:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"2.3、悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java 中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:6:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"2.4、乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和 CAS 算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java 中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:7:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"2.5、如何选择悲观 / 乐观锁？ 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 三、Redis 实现悲观锁 在商品秒杀活动活动中，流量峰值相对平常时的流量是高出非常多的。使用 Redis 实现悲观锁机制，可以解决商品库存脏读的问题。 初始化库存： public function stockInit() { $key = \"stock\"; $stockInit = 699; //清空所有缓存 Cache::clear(); Cache::store('redis')-\u003eclear(); //写入库存初始值 Cache::store('redis')-\u003eset($key, $stockInit); echo 'stock Init'; } ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:8:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"3.1、悲观锁实现（一）非最佳实践 看似符合逻辑的商品秒杀： public function flashSale() { $key = \"stock\"; $lockSuffix = \"_lock\"; //判断库存锁是否存在 while (Cache::get($key . $lockSuffix) == true) { // 存在锁定则等待 usleep(200000); } //库存上锁 Cache::store('redis')-\u003eset($key . $lockSuffix, 1, 30); //获取库存值 $stock = Cache::store('redis')-\u003eget($key); //减库存 if ($stock \u003e 0) { $temp = $stock; $stock -= 1; } else { //打开库存锁 Cache::store('redis')-\u003eset($key . $lockSuffix, false); return \"已售罄\"; } Cache::store('redis')-\u003eset($key, $stock); //打开库存锁 Cache::store('redis')-\u003eset($key . $lockSuffix, false); return \"恭喜，您抢到了第 {$temp}个库存！\"; } 实测 150 线程并发，异常率 0%，虽然引用了锁机制，看似符合逻辑的锁机制，但仍会有极低的概率脏读，原因无他，有 N 个线程同时抢到了锁。虽然概率低，但线程一多仍然会脏读。所以需要改用 redis 原生支持的 setnx 来保证只有一个线程抢到了锁。 如下，两个线程同时抢到了第 80 个库存： ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:9:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["PHP","Redis"],"content":"3.2、悲观锁实现（二） setnx 是 set if not exists 的简写，在 key 不存在时等价于 set，如果 key 存在，则不更新缓存内容，且返回 false。使用这个特性，可以保证锁只有一个线程抢到了。 使用 redis setnx 实现悲观锁的商品秒杀： public function flashSale() { $redisConifg = config('cache.redis'); //获取当前模块下的config文件夹中的cache文件的redis配置数组 $redis = Cache::connect($redisConifg); //获取thinkPHP官方封装的Redis Cache对象 $handler = Cache::connect($redisConifg)-\u003ehandler();//获取php redis扩展原生redis对象 https://github.com/phpredis/phpredis $key = \"stock\";//商品库存缓存名 $lockSuffix = \"_lock\";//商品库存锁后缀名 $timeOut = 10; //库存锁过期时间 //抢库存锁 while ($handler-\u003eset($key . $lockSuffix, 1, ['nx', 'ex' =\u003e $timeOut]) == false) { // 没有抢到则等待 usleep(20000); } //当前线程抢到库存锁了 //获取库存值 $stock = $redis-\u003eget($key); //减库存 if ($stock \u003e 0) { $temp = $stock; $stock -= 1; } else { //删除库存锁 $redis-\u003erm($key . $lockSuffix); return \"已售罄\"; } //更新库存值 $redis-\u003eset($key, $stock); //删除库存锁 $redis-\u003erm($key . $lockSuffix); return \"恭喜，您抢到了第 {$temp}个库存！\"; } 150 线程并发测试后，并没有发现有异常情况了。根据实际业务需求，可以增加等待超时机制。 四、REF https://redis.io/commands/set https://github.com/phpredis/phpredis#set https://www.jianshu.com/p/a1ebab8ce78a https://blog.csdn.net/qq_34337272/article/details/81072874 ","date":"2020-08-30","objectID":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html:10:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/posts/thinkphp-5.1-how-to-implement-pessimistic-locking-using-redis.html"},{"categories":["Nginx"],"content":"负载均衡是高可用网络基础架构的关键组件，通常用于将工作负载分布到多个服务器来提高网站、应用、数据库或其他服务的性能和可靠性。 如果手上没有足够数量的公网云服务器，本地使用虚拟机环境搭建集群做负载均衡测试是最省钱的做法，也方便做快照恢复。 一、准备工作 我使用 VMware 在本地部署了 3 台 CentOS 服务器作为测试环境，统一安装宝塔编译环境（LNMP）。虚拟机的网络配置不是本文的重点，就不再赘述了。 二、检查环境 这 3 台 CentOS 服务器编译好环境之后分别添加一个网站，检查 web 服务是否正常，并修改其中 2 台的默认模板文件，方便在测试负载均衡效果时区分是哪台服务器提供的 web 服务。 本次测试的三台服务器 IP 地址分别为 192.168.150.10（负载均衡 主服务器） 192.168.150.20（业务服务器） 192.168.150.30（业务服务器） 三、部署网站 测试域名为 t10.com，因此需要在宿主机上将此域名解析到负载均衡主服务器（192.168.150.10）。 修改宿主机的 hosts 文件（路径：C:\\Windows\\System32\\drivers\\etc\\hosts），添加以下内容。 192.168.150.10 t10.com 分别在三台服务器上添加网站后，将 t10.com 这个域名解析到网站上。 如下图示例 在负载均衡主服务器的 server 语法块下，也就是网关的配置文件中加入一条转发规则 location / { # 转发至负载均衡服务器 proxy_pass http://fzjh; proxy_connect_timeout 2s; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } 在负载均衡主服务器的 nginx 配置文件的 http 语法块下（与 server 同级别）加入规则 upstream fzjh { # least_conn; # ip_hash; server 192.168.150.20:80 weight=1; server 192.168.150.30:80 weight=1; # server 192.168.150.30:80 backup; } 保存配置文件后重启 nginx，此时访问 t10.com 时会自动分发至两台业务服务器（192.168.150.20、192.168.150.30）。 四、常用的 nginx 负载均衡算法 ","date":"2020-08-24","objectID":"/posts/load-balancing-based-on-nginx.html:0:0","series":null,"tags":["负载均衡","高可用"],"title":"基于 nginx 实现的负载均衡","uri":"/posts/load-balancing-based-on-nginx.html"},{"categories":["Nginx"],"content":"1、轮询（默认） 轮询即 Round Robin，根据 Nginx 配置文件中的顺序，依次把客户端的 Web 请求分发到不同的后端服务器。 http{ upstream sampleapp { server ...; server ...; } .... server{ listen 80; ... location / { proxy_pass http://sampleapp; } } ","date":"2020-08-24","objectID":"/posts/load-balancing-based-on-nginx.html:1:0","series":null,"tags":["负载均衡","高可用"],"title":"基于 nginx 实现的负载均衡","uri":"/posts/load-balancing-based-on-nginx.html"},{"categories":["Nginx"],"content":"2、least_conn（最少连接） Web 请求会被转发到连接数最少的服务器上。 http{ upstream sampleapp { least_conn; server ...; server ...; } .... server{ listen 80; ... location / { proxy_pass http://sampleapp; } } ","date":"2020-08-24","objectID":"/posts/load-balancing-based-on-nginx.html:2:0","series":null,"tags":["负载均衡","高可用"],"title":"基于 nginx 实现的负载均衡","uri":"/posts/load-balancing-based-on-nginx.html"},{"categories":["Nginx"],"content":"3、ip_hash（IP 地址哈希） 前述的两种负载均衡方案中，同一客户端连续的 Web 请求可能会被分发到不同的后端服务器进行处理，因此如果涉及到会话 Session，那么会话会比较复杂。常见的是基于数据库的会话持久化。要克服上面的难题，可以使用基于 IP 地址哈希的负载均衡方案。这样的话，同一客户端连续的 Web 请求都会被分发到同一服务器进行处理。 http{ upstream sampleapp { ip_hash; server ...; server ...; } .... server{ listen 80; ... location / { proxy_pass http://sampleapp; } } ","date":"2020-08-24","objectID":"/posts/load-balancing-based-on-nginx.html:3:0","series":null,"tags":["负载均衡","高可用"],"title":"基于 nginx 实现的负载均衡","uri":"/posts/load-balancing-based-on-nginx.html"},{"categories":["Nginx"],"content":"4、基于权重的负载均衡算法 基于权重的负载均衡即 Weighted Load Balancing，这种方式下，我们可以配置 Nginx 把请求更多地分发到高配置的后端服务器上，把相对较少的请求分发到低配服务器。 http{ upstream sampleapp { server ... weight=1; server ... weight=1; } .... server{ listen 80; ... location / { proxy_pass http://sampleapp; } } backup 即为备机，当所有业务服务器都宕机时，请求会自动转发至备机； 多台业务服务器的代码同步可以使用 rsync 解决。 ","date":"2020-08-24","objectID":"/posts/load-balancing-based-on-nginx.html:4:0","series":null,"tags":["负载均衡","高可用"],"title":"基于 nginx 实现的负载均衡","uri":"/posts/load-balancing-based-on-nginx.html"},{"categories":["软件"],"content":"内存盘就是将一部分硬盘当硬盘使用，由于内存速度很快，利用了这个特性设置虚拟内存硬盘，在一些特定场景和需求下是非常有帮助的。 界面简洁，原生支持中文，支持在创建时自动创建目录，能够满足需求。 官网：https://www.softperfect.com/products/ramdisk 度盘：https://pan.baidu.com/s/1z9elPiX103hloTwXj-NW5w 提取码：kcik ","date":"2020-08-07","objectID":"/posts/ram-disk-tool.html:0:0","series":null,"tags":["内存盘"],"title":"SoftPerfect RAM Disk 内存盘工具","uri":"/posts/ram-disk-tool.html"},{"categories":["折腾"],"content":"在开发 API 完成后，测试的环节中免不了要进行压力测试。postman 的 runner 只支持串行化测试，不支持并发。jmeter 需要在本机上安装 JDK，且需要配置好 JAVA 环境变量，安装略显复杂，软件界面对小白并不友好，非开箱即用的工具。权衡之后，我个人推荐任何对压力测试毫无经验的人优先选择使用 Linux 环境下的 ab（apache benchmark）压测工具进行压力测试。 # 安装 ab yum -y install httpd-tools # 显示可选的参数列表及说明 ab -help # 测试 # c：并发数量 # n：总计请求数量 ab -c 100 -n 10000 https://yourdomain.com/getUserInfo?id=36\u0026token=6vPgUT0gG1RyzBRKxYsSNBgwFwM1mQLz 耐心等待测试完毕，以下是测试报告 Server Software: nginx Server Hostname: yourdomain.com Server Port: 443 SSL/TLS Protocol: TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128 Document Path: getUserInfo?id=36\u0026token=6vPgUT0gG1RyzBRKxYsSNBgwFwM1mQLz Document Length: 41 bytes Concurrency Level: 200 Time taken for tests: 187.546 seconds Complete requests: 30000 Failed requests: 53 (Connect: 0, Receive: 0, Length: 53, Exceptions: 0) Write errors: 0 Non-2xx responses: 47 Total transferred: 11695535 bytes HTML transferred: 1235123 bytes Requests per second: 159.96 [#/sec] (mean) Time per request: 1250.304 [ms] (mean) Time per request: 6.252 [ms] (mean, across all concurrent requests) Transfer rate: 60.90 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 420 1078.0 286 63684 Processing: 50 640 996.0 609 69848 Waiting: 50 625 248.5 608 1796 Total: 217 1060 1452.9 951 69848 Percentage of the requests served within a certain time (ms) 50% 951 # 50%的请求在951ms内返回 66% 1074 75% 1152 80% 1205 90% 1391 # 90%的请求在1391ms内返回 95% 1660 98% 2043 99% 2928 # 99%的请求在2928ms内返回 100% 69848 (longest request) 比较重要的也就是最后这部分的 percent line 了，jmeter 只有 90%、95%、99% 这三个量级，ab 从 50% 到 100% 都有。 ","date":"2020-06-13","objectID":"/posts/stress-test-the-api-using-apache-benchmark.html:0:0","series":null,"tags":["ab"],"title":"使用 apache benchmark 对 API 进行压力测试","uri":"/posts/stress-test-the-api-using-apache-benchmark.html"},{"categories":["Nginx"],"content":"自动切割目标目录下的所有 .log 文件，即使新建站点，产生新的日志需要切割，也无需修改脚本。 #!/bin/bash #自动版日志切割 无需设置 #存放 .log 日志文件的路径 log_files_path=\"/home/wwwlogs\" #存放切割后日志的文件夹路径 log_files_dir=${log_files_path}/$(date -d \"yesterday\" +\"%Y\")/$(date -d \"yesterday\" +\"%m\") #nginx的路径 nginx_sbin=\"/usr/local/nginx/sbin/nginx\" #保存多少天内的日志 save_days=30 ############################################ # 请勿修改此脚本下面的任何内容！！！！！！！！！ # ############################################ mkdir -p $log_files_dir cd $log_files_path #cut nginx log files for d in `ls -f *.log`;do mv ${log_files_path}${d} ${log_files_dir}/$(date -d \"yesterday\" +\"%Y%m%d\")_$d done #delete 30 days ago nginx log files find $log_files_path -mtime +$save_days -exec rm -rf {} \\; $nginx_sbin -s reload ","date":"2020-03-28","objectID":"/posts/nginx-log-auto-chunk-script.html:0:0","series":null,"tags":["日志"],"title":"Nginx 日志全自动切割脚本","uri":"/posts/nginx-log-auto-chunk-script.html"},{"categories":["Linux"],"content":"刚接触 Linux 时最怕的就是 SSH 远程登录 Linux VPS 编译安装程序时（比如安装 lnmp）网络突然断开，或者其他情况导致不得不与远程 SSH 服务器链接断开，远程执行的命令也被迫停止，只能重新连接，重新运行。使用 screen 可以解决这个问题。 一、screen 是什么？ Screen 是一个可以在多个进程之间多路复用一个物理终端的全屏窗口管理器。Screen 中有会话的概念，用户可以在一个 screen 会话中创建多个 screen 窗口，在每一个 screen 窗口中就像操作一个真实的 telnet/SSH 连接窗口那样。 二、如何安装 screen 命令？ 除部分精简的系统或者定制的系统大部分都安装了 screen 命令，如果没有安装，CentOS 系统可以执行：yum install screen ； CentOS 8 上移除了 screen，需要安装 epel 后安装 screen 执行：yum install screen ； Debian/Ubuntu 系统执行：apt-get install screen 。 PS：安装 epel ：https://blog.keyboardman.fun/linux/729.html 三、screen 命令使用方法？ ","date":"2020-03-28","objectID":"/posts/ssh-remote-session-management-tool-screen-tutorial.html:0:0","series":null,"tags":[],"title":"SSH 远程会话管理工具 – screen使用教程","uri":"/posts/ssh-remote-session-management-tool-screen-tutorial.html"},{"categories":["Linux"],"content":"3.1 创建 screen 会话 可以先执行：screen -S lnmp ，screen 就会创建一个名字为 lnmp 的会话。 VPS 侦探 https://www.vpser.net/ ","date":"2020-03-28","objectID":"/posts/ssh-remote-session-management-tool-screen-tutorial.html:1:0","series":null,"tags":[],"title":"SSH 远程会话管理工具 – screen使用教程","uri":"/posts/ssh-remote-session-management-tool-screen-tutorial.html"},{"categories":["Linux"],"content":"3.2 暂时离开，保留 screen 会话中的任务或程序 当需要临时离开时（会话中的程序不会关闭，仍在运行）可以用快捷键 Ctrl+a d (即按住 Ctrl，依次再按 a,d) ","date":"2020-03-28","objectID":"/posts/ssh-remote-session-management-tool-screen-tutorial.html:2:0","series":null,"tags":[],"title":"SSH 远程会话管理工具 – screen使用教程","uri":"/posts/ssh-remote-session-management-tool-screen-tutorial.html"},{"categories":["Linux"],"content":"3.3 恢复 screen 会话 当回来时可以再执行执行：screen -r lnmp 即可恢复到离开前创建的 lnmp 会话的工作界面。如果忘记了，或者当时没有指定会话名，可以执行：screen -ls screen 会列出当前存在的会话列表 ","date":"2020-03-28","objectID":"/posts/ssh-remote-session-management-tool-screen-tutorial.html:3:0","series":null,"tags":[],"title":"SSH 远程会话管理工具 – screen使用教程","uri":"/posts/ssh-remote-session-management-tool-screen-tutorial.html"},{"categories":["Linux"],"content":"3.4 关闭 screen 的会话 执行：exit ，会提示：[screen is terminating]，表示已经成功退出 screen 会话。 四、远程演示 首先演示者先在服务器上执行 screen -S test 创建一个 screen 会话，观众可以链接到远程服务器上执行 screen -x test 观众屏幕上就会出现和演示者同步。 五、常用快捷键 Ctrl+a c ：在当前 screen 会话中创建窗口 Ctrl+a w ：窗口列表 Ctrl+a n ：下一个窗口 Ctrl+a p ：上一个窗口 Ctrl+a 0-9 ：在第 0 个窗口和第 9 个窗口之间切换 ","date":"2020-03-28","objectID":"/posts/ssh-remote-session-management-tool-screen-tutorial.html:4:0","series":null,"tags":[],"title":"SSH 远程会话管理工具 – screen使用教程","uri":"/posts/ssh-remote-session-management-tool-screen-tutorial.html"},{"categories":["Mark"],"content":"你还在发起 ajax 请求时在 data 里写上一大堆参数，或是手动拼接参数字符串吗？ var url = \"http://example.com/...\"; $.ajax({ url: url, type: 'POST',//使用post方式提交 data:$(\"form\").serialize(),//使用jQuery选择器获取form标签中的数据，并序列化 success: function (data) { if (data['code'] == 1) { // ... } else if(data['code'] == 0) { // ... } else { // ... } } }); jQuery Yes！ ","date":"2020-02-27","objectID":"/posts/how-to-gracefully-launch-an-ajax-request-with-jquery.html:0:0","series":null,"tags":["JQuery","Ajax"],"title":"JQuery 如何优雅的发起一个 ajax 请求","uri":"/posts/how-to-gracefully-launch-an-ajax-request-with-jquery.html"},{"categories":["News"],"content":"Microsoft 将于 2020 年 1 月 14 日对 Windows 7 终止支持 一个时代落幕了。 ","date":"2020-01-14","objectID":"/posts/goodbye-windows-7.html:0:0","series":null,"tags":[],"title":"再见 Windows7","uri":"/posts/goodbye-windows-7.html"},{"categories":["闲聊"],"content":"在调试设备的时候需要来回切换笔记本电脑无线网卡的静态 IP 地址，非常麻烦。所以写了两个 .bat 的批处理脚本，希望对你有帮助。 一、DHCP 将如下内容保存为 set_WLAN_DHCP.bat 文件 设置为自动获取IP/DNS @echo off % 设置UTF-8 防止中文乱码 % chcp 65001 echo 设置IP信息自动获取 netsh interface ip set address \"WLAN\" source=dhcp echo 设置DNS自动获取 netsh interface ip set dnsservers \"WLAN\" source=dhcp echo 完成 pause 二、静态 IP 将如下内容保存为 set_WLAN_StaticIP.bat 文件 设置为静态IP/DNS @echo off % 设置UTF-8 防止中文乱码 % chcp 65001 echo 设置IP、子网掩码及网关 netsh interface ip set address name=\"WLAN\" static 192.168.11.119 255.255.255.0 192.168.11.1 1 \u003enul echo 设置主DNS netsh interface ip set dns name=\"WLAN\" static 192.168.11.1 \u003enul echo 设置副DNS netsh interface ip add dns name=\"WLAN\" 114.114.114.114 2 \u003enul echo 完成 pause PS：两个脚本中的 “WLAN” 是因为我的笔记本无线网卡名称为 WLAN ","date":"2019-11-22","objectID":"/posts/setting-up-windows-ip-dns-using-bat-scripts.html:0:0","series":null,"tags":[],"title":"Windows 使用 .bat 批处理脚本设置 IP、DNS ","uri":"/posts/setting-up-windows-ip-dns-using-bat-scripts.html"},{"categories":["软件"],"content":"Snipaste 是一个简单但强大的截图工具，同时可以让你将截图贴到屏幕上！下载并打开 Snipaste，按下 F1 来开始截图，再按 F3，截图就在桌面置顶显示了。就这么简单！而且它提供的截图操作功能也相当全面，足够当成一款截图工具使用。当前支持 windows / mac 平台。 官网：https://www.snipaste.com 中文官网：https://zh.snipaste.com 你还可以将剪贴板里的文字或者颜色信息转化为图片窗口，并且将它们进行缩放、旋转、翻转、设为半透明，甚至让鼠标能穿透它们！如果你是程序员、设计师，或者是大部分工作时间都在电脑前，贴图功能将改变你的工作方式、提升工作效率。 Snipaste 使用很简单，但同时也有一些较高级的用法可以进一步提升你的工作效率。感兴趣的话，请抽空读一读用户手册。 Snipaste 是免费软件，它也很安全，没有广告、不会扫描你的硬盘、更不会上传用户数据，它只做它应该做的事。 ","date":"2019-10-18","objectID":"/posts/snipaste-simple-but-powerful-snipping-tool.html:0:0","series":null,"tags":[],"title":"Snipaste - 简单但强大的截图工具","uri":"/posts/snipaste-simple-but-powerful-snipping-tool.html"},{"categories":["Python"],"content":"没有多进程，没有任何黑科技的裸爬虫。练手用，爬虫获取到的数据皆为非敏感且公开的用户信息。 一、思路 在 GitHub 上已经有网易云音乐的 node.js API（GitHub：https://github.com/Binaryify/NeteaseCloudMusicApi）。根据这个库提供的信息，可以很轻易的获取到网易云音乐获取某个用户的粉丝信息接口的参数（接口限制只能获取 100 个），进而继续获取这 100 个粉丝的粉丝… 简单的几层循环嵌套就能很轻易的拿到十万级到百万级的用户数据（非敏感用户信息）。 二、参数加密流程分析 __getFormData(data, __get_random_str()) 参数1：data是dict数据，包含了表单的各个字段和数据 参数2：16位的随机字符串 最终return的是一个dict，包含了params和encSecKey两个参数 params __get_encText(args1, random16str) 参数1：args1是__getFormData函数的参数1，是dict数据，包含了表单的各个字段和数据 参数2：random16str是__getFormData函数的参数2，是一个16位的随机字符串 最终返回的是将参数加密后产生的params __AES_encrypt(args1, args4) 参数1：args1是__get_encText函数的参数1，是dict数据，包含了表单的各个字段和数据 参数2：arg4是一个固定参数 最终返回的是将参数使用AES CBC加密后再进行一次base64加密产生的字符串 使用__AES_encrypt函数首先加密一次参数是（args1，args4）得到一个加密的字符串 在使用加密过一次的字符串作为参数1，和__get_encText函数传入的参数2 random16str 这个随机16位的字符串作为参数2继续加密1次 最终得到params encSecKey __get_encSecKey(random16str) 参数1：random16str是__getFormData函数的参数2，是一个16位的随机字符串 最终返回的是通过随机字符串产生的encSecKey 固定参数arg2 固定参数arg3 通过固定算法，使用随机16位的字符串random16str与这两个固定参数产生encSecKey 三、源码 ","date":"2019-09-12","objectID":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html:0:0","series":null,"tags":[],"title":"我用一天时间“偷了”网易云音乐50W+用户信息","uri":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html"},{"categories":["Python"],"content":"common.py （需要用到的函数） import base64 from Crypto.Cipher import AES import random import codecs import requests from fake_useragent import UserAgent def __AES_encrypt(text, key): ''' 获取到加密后的数据 :param text: 首先CBC加密方法，text必须位16位数据 :param key: 加密的key :return: 加密后的字符串 ''' iv = \"0102030405060708\" pad = 16 - len(text) % 16 if isinstance(text, str): text = text + pad * chr(pad) else: text = text.deocde(\"utf-8\") + pad * chr(pad) aes = AES.new(key=bytes(key, encoding=\"utf-8\"), mode=2, IV=bytes(iv, encoding=\"utf-8\")) res = aes.encrypt(bytes(text, encoding=\"utf-8\")) res = base64.b64encode(res).decode(\"utf-8\") return res def __get_encText(args1, random16str): args4 = \"0CoJUm6Qyw8W8jud\" encText = __AES_encrypt(args1, args4) encText = __AES_encrypt(encText, random16str) return encText def __get_encSecKey(random16str): '''通过查看js代码，获取encSecKey''' arg2 = \"010001\" arg3 = \"00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7\" text = random16str[::-1] rs = int(codecs.encode(text.encode('utf-8'), 'hex_codec'), 16) ** int(arg2, 16) % int(arg3, 16) return format(rs, 'x').zfill(256) def __get_random_str(): '''这是16位的随机字符串''' str_set = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\" random_str = \"\" for i in range(16): index = random.randint(0, len(str_set) - 1) random_str += str_set[index] return random_str def __getFormData(args1, random16str): '''获取到提交的数据''' data = {\"params\": __get_encText(args1, random16str), \"encSecKey\": __get_encSecKey(random16str)} return data def __getFans(userID): userDict = {} userID = str(userID) # userID=\"6177307\" data = '{\"userId\":\"' + userID + '\",\"time\":\"-1\",\"limit\":\"104334\",\"csrf_token\": \"\"}' formdata = __getFormData(data, __get_random_str()) ua = UserAgent() session = requests.Session() headers = {} headers[\"content-type\"] = \"application/x-www-form-urlencoded\" headers[\"user-agent\"] = ua.random headers[\"referer\"] = \"https://music.163.com/\" response = session.post(url='https://music.163.com/weapi/user/getfolloweds', headers=headers, data=formdata) results = response.json() # print(response.status_code) results = results.get('followeds') for one in results: userDict[one.get('userId')] = one # print(one.get('userId')) # print(str(one)) return userDict ","date":"2019-09-12","objectID":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html:1:0","series":null,"tags":[],"title":"我用一天时间“偷了”网易云音乐50W+用户信息","uri":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html"},{"categories":["Python"],"content":"main.py （主程序） # -*- coding: utf-8 -*- import common AllData = {} data = common.__getFans(6177307) AllData.update(data) sum = 0 for item in data: temp = common.__getFans(item) AllData.update(temp) for item1 in temp: temp2 = common.__getFans(item1) AllData.update(temp2) for item2 in temp2: sum += 1 print(sum) temp3 = common.__getFans(item2) AllData.update(temp3) with open('fans.txt', 'a', encoding='utf-8') as f: for one in AllData.items(): f.write(str(one) + '\\n') print(str(one)) 四、数据 我把 main.py 放到了服务器上运行，跑到程序结束大概用了 24 小时左右后看了一下存储的文本有 50W 左右的用户数据（不含敏感信息），如下。 (83543823, {'py': 'mjdtjst', 'time': 1510758264852, 'userId': 83543823, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '名劍動天倦收天', 'avatarUrl': 'http://p1.music.126.net/uocXBF145t-_V0pLWDwv0w==/3272146604393759.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 13, 'remarkName': None, 'follows': 19, 'authStatus': 0, 'userType': 0, 'vipType': 0, 'signature': '天下若倾，尚有儒门一手擎天！', 'vipRights': None, 'eventCount': 1, 'playlistCount': 5}) (305937375, {'py': 'ttqsunny', 'time': 1510751111003, 'userId': 305937375, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '甜甜圈sunny', 'avatarUrl': 'http://p1.music.126.net/v9iyq-6I1WC96R7SlbKvXQ==/3420580709664324.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 4, 'remarkName': None, 'follows': 30, 'authStatus': 0, 'userType': 0, 'vipType': 11, 'signature': None, 'vipRights': {'associator': {'vipCode': 100, 'rights': True}, 'musicPackage': None, 'redVipAnnualCount': -1}, 'eventCount': 0, 'playlistCount': 3}) (359743222, {'py': 'xxwmhjl-_', 'time': 1509411979309, 'userId': 359743222, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '醒醒我们回家了-_', 'avatarUrl': 'http://p1.music.126.net/KcAVTPDSC8MrKaFB9_Vd9g==/109951163985306640.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 4, 'remarkName': None, 'follows': 9, 'authStatus': 0, 'userType': 0, 'vipType': 0, 'signature': '身邪不怕影子正', 'vipRights': None, 'eventCount': 0, 'playlistCount': 7}) (285736292, {'py': 'gzydyzj-', 'time': 1509016136464, 'userId': 285736292, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '孤舟夜灯一掌剑-', 'avatarUrl': 'http://p1.music.126.net/SsOPGfkTUM0dSEkOuvHfHQ==/109951163351011922.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 13, 'remarkName': None, 'follows': 33, 'authStatus': 0, 'userType': 0, 'vipType': 0, 'signature': '寂寞候忘了向先生说告辞！', 'vipRights': None, 'eventCount': 39, 'playlistCount': 23}) (79969440, {'py': 'cj', 'time': 1508992729596, 'userId': 79969440, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '尺疾', 'avatarUrl': 'http://p1.music.126.net/SLdf4abndYLV4Gq8eXbK8w==/109951163788127476.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 9, 'remarkName': None, 'follows': 17, 'authStatus': 0, 'userType': 0, 'vipType': 11, 'signature': '绝情莫过绝音讯，断爱无非断感情', 'vipRights': {'associator': {'vipCode': 100, 'rights': True}, 'musicPackage': None, 'redVipAnnualCount': -1}, 'eventCount': 0, 'playlistCount': 6}) (413801433, {'py': 'Aresfx', 'time': 1508811460203, 'userId': 413801433, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': 'Ares风雪', 'avatarUrl': 'http://p1.music.126.net/B1qPhuJvDGuAkQxGvNkxoA==/18828037115798229.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 5, 'remarkName': None, 'follows': 32, 'authStatus': 0, 'userType': 0, 'vipType': 0, 'signature': None, 'vipRights': None, 'eventCount': 6, 'playlistCount': 11}) (385103142, {'py': 'gyw', 'time': 1508734902599, 'userId': 385103142, 'mutual': False, 'followed': False, 'accountStatus': 0, 'nickname': '归于无', 'avatarUrl': 'http://p1.music.126.net/HjaQMktAYh5dDuf009Fv5A==/18588343580852886.jpg', 'gender': 1, 'expertTags': None, 'experts': None, 'followeds': 1, 'remarkName': None, 'follows': 8, 'authStatus': 0, 'userType': 0, 'vipType': 0, 'signature': None, 'vipRights': None, 'eventCount': 0, 'play","date":"2019-09-12","objectID":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html:2:0","series":null,"tags":[],"title":"我用一天时间“偷了”网易云音乐50W+用户信息","uri":"/posts/i-spent-a-day-stealing-50w-user-information-from-netease-cloud-music.html"},{"categories":["折腾"],"content":"比之 gitLab，gogs 在资源占用和性能上的优势是非常明显的。十分适合配置并不高的服务器搭建使用。官网是有搭建好的 demo，可以在线体验一下。 本文命令于 CentOS7 系统中测试通过。 官网：https://gogs.io Github：https://github.com/gogs/gogs 文档：https://gogs.io/docs/installation/install_from_binary 我比较习惯通过二进制文件安装 gogs，所以本文以二进制安装为例说明。有兴趣使用其他方法的朋友们可以自行尝试。 一、添加 nginx 站点 将域名解析到你要安装的服务器 IP server { listen 80; server_name git.yourdomain.top;#域名修改为你自己的 location / { proxy_pass http://localhost:3000; } access_log /home/git/yourdomain_access.log; } 二、安装并配置 gogs adduser git #创建用户 git passwd git #设置git用户的密码 groupadd git #创建用户组 git usermod -G git git #将git用户添加到git用户组中 su git #切换到git用户shell cd ~ #进入/home/git wget https://dl.gogs.io/0.11.91/gogs_0.11.91_linux_amd64.zip #下载gogs unzip gogs_0.11.91_linux_amd64.zip #解压gogs压缩包 su #切回root用户 cp /home/git/gogs/scripts/systemd/gogs.service /usr/lib/systemd/system/ #复制service文件 systemctl enable gogs.service #启用gogs服务（开机自启） systemctl start gogs #启动gogs systemctl status gogs #查看gogs运行状态 注意事项 如果你使用的是端口访问的方式，请在主机 IDC 控制台（阿里云或腾讯云等）设置对应的安全组规则，开放入方向的 3000 端口，并且在宝塔的安全设置中开放 3000 端口 创建数据库请使用 utf-8 编码格式，不要 utf8mb4 。 ","date":"2019-09-02","objectID":"/posts/building-a-private-gitserver-with-gogs.html:0:0","series":null,"tags":[],"title":"使用 Gogs 搭建私有的 GitServer","uri":"/posts/building-a-private-gitserver-with-gogs.html"},{"categories":["软件"],"content":"软件体积不大，底层由 VC++ 编写，用起来也是十分流畅。功能也是十分多样，我也一直在用这款播放器。安利一下 下载地址 https://daumpotplayer.com/download https://potplayer.daum.net （这个貌似已经打不开了） 度盘：https://pan.baidu.com/s/1CO5Xq51aAi5b7_9Y-9xKYQ 提取码：i018 ","date":"2019-08-31","objectID":"/posts/potplayer-small-and-powerful-video-player.html:0:0","series":null,"tags":[],"title":"potplayer - 小巧、功能强大的视频播放器","uri":"/posts/potplayer-small-and-powerful-video-player.html"},{"categories":["Linux"],"content":"Secure Shell（安全外壳协议，简称 SSH）是一种加密的网络传输协议，可在不安全的网络中为网络服务提供安全的传输环境。SSH 通过在网络中创建安全隧道来实现 SSH 客户端与服务器之间的连接。虽然任何网络服务都可以通过 SSH 实现安全传输，SSH 最常见的用途是远程登录系统，通常利用 SSH 来传输命令行界面和远程执行命令。 在设计上，SSH 是 Telnet 和非安全 shell 的替代品。Telnet 和 Berkeley rlogin、rsh、rexec 等协议采用明文传输，使用不可靠的密码，容易遭到监听、嗅探和中间人攻击 [5]。SSH 旨在保证非安全网络环境（例如互联网）中信息加密完整可靠。 一、Windows 远程登录 Linux 下载 git bash ，方便执行 ssh 命令。 官网：https://git-scm.com ","date":"2019-08-25","objectID":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html:0:0","series":null,"tags":["ssh"],"title":"如何使用 SSH 远程登录 Linux 服务器","uri":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html"},{"categories":["Linux"],"content":"1.1、生成 SSH 密钥对 在 windows 中的 cmd 中输入（使用 git bash 客户端输入也可以） ssh-keygen 连摁 3 个回车即可。 生成的秘钥文件在 C:\\Users\\Administrator.ssh 文件夹下 ","date":"2019-08-25","objectID":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html:1:0","series":null,"tags":["ssh"],"title":"如何使用 SSH 远程登录 Linux 服务器","uri":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html"},{"categories":["Linux"],"content":"1.2、添加公钥文件至 Linux 服务器 打开 git bash，输入以下命令 # 将192.168.1.1替换为Linux服务器的真实IP地址 # root是最高权限用户，可以替换为你想要远程登录的某个用户 # ssh端口默认是22 ssh-copy-id root@192.168.1.1 -p 22 之后的提示输入 yes ，再输入 Linux 的用户密码就可以了。（输入密码时客户端不会显示任何输入提示） ","date":"2019-08-25","objectID":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html:2:0","series":null,"tags":["ssh"],"title":"如何使用 SSH 远程登录 Linux 服务器","uri":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html"},{"categories":["Linux"],"content":"1.3、登陆 打开 git bash，输入以下命令就可以直接进入远程 Linux 的 Shell 了。 # 将192.168.1.1替换为Linux服务器的真实IP地址 # root是最高权限用户，可以替换为你想要远程登录的某个用户 # ssh端口默认是22 ssh root@192.168.1.1 -p 22 ","date":"2019-08-25","objectID":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html:3:0","series":null,"tags":["ssh"],"title":"如何使用 SSH 远程登录 Linux 服务器","uri":"/posts/how-to-use-ssh-to-remotely-log-in-to-a-linux-server.html"},{"categories":["软件"],"content":"HashCheck 全称 HashCheck Shell Extension，安装后添加到属性窗口中，是一款非常小巧快速的文件校验工具，而且 HashCheck 在 Github 上开放了源代码。软件体积小，文件 hash 值计算速度快。安装后迁入资源管理器的属性中，查看文件 hash 值十分方便。 下载地址 官网：http://code.kliu.org/hashcheck Github：https://github.com/gurnec/HashCheck 度盘：https://pan.baidu.com/s/11yLvdMRYHhFncVMvyumO9Q 提取码：1k4a ","date":"2019-08-21","objectID":"/posts/hashcheck-a-small-and-useful-file-verification-tool.html:0:0","series":null,"tags":[],"title":"HashCheck - 一款小巧、实用的文件校验工具","uri":"/posts/hashcheck-a-small-and-useful-file-verification-tool.html"},{"categories":["Python"],"content":"在写脚本的过程中无法避免的需要一些配置性的信息，但是又不想写死在 python 文件中，刚好 python 的 configparser 模块实现了以上的功能，可以用来操作 .ini 配置文件。 一、新建配置文件 在 python 项目中新建一个 conf 文件夹，并在此文件夹下创建一个 conf.ini 的文件用于存储配置信息。 文件路径：./conf/conf.ini 文件内容： [EditionInfo] version = 1.0.0 二、源码 import configparser import os import sys def get_path(config_name): \"\"\"获取路径 返回运行本函数的文件绝对路径+config_name determine if application is a script file or frozen exe \"\"\" if getattr(sys, 'frozen', False): application_path = os.path.dirname(sys.executable) elif __file__: application_path = os.path.dirname(__file__) config_path = os.path.join(application_path, config_name) return config_path # 读取conf.ini configPath = \"conf\\\\conf.ini\" config = configparser.ConfigParser() config.read_file(open(get_path(configPath), encoding='UTF-8')) # 获取conf中的section sections = config.sections() # 获取第一个section中的所有option paramList = config.options(sections[0]) # 修改version的值 config.set(sections[0], paramList[0], \"1.0.1\") config.write(open(configPath, \"w\")) # 获取version的值 方法1 version = str(config.get(sections[0], paramList[0])) # 获取version的值 方法2 # version = str(config.get(\"EditionInfo\", \"version\")) print(version) 可能有心人会注意到 get_path 的方法有点怪异，那是因为在写 python 程序中，有可能需要获取当前运行脚本的路径。打包成 exe 的脚本和直接运行地脚本在获取路径上稍微有点不同。 get_path 的方法参考了这篇博客：https://www.cnblogs.com/pzxbc/archive/2012/03/18/2404695.html REF python中获取打包成执行文件(exe)和脚本运行文件的路径 ：https://www.cnblogs.com/pzxbc/archive/2012/03/18/2404695.html ","date":"2019-08-19","objectID":"/posts/manipulating-configuration-files-with-the-python-configparser-library.html:0:0","series":null,"tags":[],"title":"Python 使用 configparser 库操作配置文件","uri":"/posts/manipulating-configuration-files-with-the-python-configparser-library.html"},{"categories":["Linux"],"content":"一直学下 Shell 脚本的，借鉴了很多大佬的脚本，靠着 Google 写「Ctrl+C / V」了个 Demo 出来。 直接放脚本，Mark 一下防止以后忘记… 复制脚本内容，保存为 *.sh 文件，运行 bash *.sh 结合查看脚本的输出情况来理解命令更佳。 #!/bin/bash :\u003c\u003cEOF 这是多行注释区块 $0 当前脚本的文件名 $n 传递给脚本或函数的参数。n 是一个数字，表示第几个参数。例如，第一个参数是$1，第二个参数是$2。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同，下面将会讲到。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前Shell进程ID。对于 Shell 脚本，就是这些脚本所在的进程ID。 EOF str=\"frist shell script\" #字符串拼接 echo \"1：do u know this is my $str\" #字符串长度 echo \"2：${#str}\" #提取子字符串 echo \"3：${str:1:4}\" #查找子字符串 查找字符 i 或 s 的位置(哪个字母先出现就计算哪个) echo \"4：`expr index \"$str\" is`\" #数组定义 array_test=(1 'A' 2 4) #获取数组元素值 echo \"5：${array_test[0]}\" echo \"6：${array_test[1]}\" echo \"7：${array_test[*]}\" echo \"8：${array_test[@]}\" #获取数组的长度 echo \"9：${#array_test[@]}\" #加法运算 val=`expr 2 + 2` echo \"10：两数之和为 : $val\" #在当前目录下生成log文件 log_path=\"test.log\" echo \"11：往当前目录下的log文件写入内容,如果文件不存在则自动创建。\" #如果不希望文件内容被覆盖，可以使用 \u003e\u003e 追加到文件末尾 echo \"It is a test\" \u003e $log_path #显示命令执行结果 echo \"12：显示命令执行结果 ls\" echo `ls` :\u003c\u003cEOF %s %c %d %f都是格式替代符 %-10s 指一个宽度为10个字符（-表示左对齐，没有则表示右对齐），任何字符都会被显示在10个字符宽的字符内 如果不足则自动以空格填充，超过也会将内容全部显示出来。 %-4.2f 指格式化为小数，其中.2指保留2位小数。 EOF #printf显示表格 echo -e \"13：printf显示表格\" printf \"%-10s %-8s %-4s\\n\" 姓名 性别 体重kg printf \"%-10s %-8s %-4.2f\\n\" 郭靖 男 66.1234 printf \"%-10s %-8s %-4.2f\\n\" 杨过 男 48.6543 printf \"%-10s %-8s %-4.2f\\n\" 郭芙 女 47.9876 # 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用 echo -e \"14：format-string 重用\" printf \"%s\\n\" abc df printf \"%s %s %s\\n\" a b c d e f g h i j #if echo \"14：if语句\" a=102 b=101 if [ $a -eq $b ] then echo '两个数相等！' elif [ $a -gt $b ] then echo 'a大于b！' else echo 'a小于b！' fi a=101 b=101 if [[ $a \u003e $b || $a == $b ]] then echo 'c大等于d' else echo 'c小于d' fi #字符串比较 c='sss' d='sss' if [[ $c == $d ]] then echo 'c和d相等！' else echo 'c和d不相等！' fi #for循环 echo -e \"15：for循环输出数组内的所有元素\" for vo in ${array_test[*]} do echo $vo done #while循环 echo -e \"15：while循环输出数组内的所有元素\" array_length=${#array_test[*]} temp=1 while [ $temp -lt $array_length ] do echo $temp let \"temp++\" done #until循环 #until 循环执行一系列命令直至条件为 true 时停止。 temp=1 echo \"16：until循环输出数组内的所有元素\" until [ $temp -eq $array_length ] do echo $temp let \"temp++\" done #读取用户输入 echo \"17：读取用户输入\" echo \u0026\u0026 read -e -p \"请输入数字 [1-4]：\" aNum case $aNum in 1) echo '你选择了 1' ;; 2) echo '你选择了 2' ;; 3) echo '你选择了 3' ;; 4) echo '你选择了 4' ;; *) echo '你没有输入 1 到 4 之间的数字' ;; esac #函数定义 调用 echo \"18：函数定义 调用\" demoFun(){ echo \"这是我的第一个 shell 函数!\" } echo \"-----函数开始执行-----\" demoFun echo \"-----函数执行完毕-----\" ","date":"2019-05-31","objectID":"/posts/getting-started-with-linux-shell-scripting.html:0:0","series":null,"tags":["shell"],"title":"Linux Shell 脚本入门","uri":"/posts/getting-started-with-linux-shell-scripting.html"},{"categories":["Linux"],"content":"在 Linux 服务器 IP 可能发生变动的情况下，使用 DDNS（Dynamic Domain Name Server / 动态域名服务）可以随时将本地服务器的 IP 更新至域名解析。DDNS 的作用和适用范围不再赘述，下面说明如何配置。 一、准备工作 注册一个 cloudflare 账号后，按照 cloudflare 给出的提示，将域名接入 cloudflare，让 cloudflare 接管你的域名。 二、获取账号 global key 打开网页：https://dash.cloudflare.com/profile 在页面下方找到【Global API Key】，点击右侧的 View 查看 Key，并保存下来 三、设置用于 DDNS 解析的二级域名 在 Cloudflare 中新建一个 A 记录，如：ddns.yourdomain.com，指向 1.1.1.1（可随意指定，如 123.123.123.123 等等，主要用于后续查看 DDNS 是否生效） 四、下载 DDNS 脚本，修改配置 wget -N --no-check-certificate https://raw.githubusercontent.com/yulewang/cloudflare-api-v4-ddns/master/cf-v4-ddns.sh vim cf-v4-ddns.sh 1.CFKEY 就是第一步获取的 global key 2.CFUSER 是登录 cloudflare 的邮箱 3.CFZONE_NAME 是你的一级域名 4.CFRECORD_NAME 则是用于 DDNS 的二级域名 5.CFTTL 是域名生效的 ttl，默认 120 即可 五、脚本授权并执行 chmod +x cf-v4-ddns.sh ./cf-v4-ddns.sh 如果脚本相关信息填写正确，输出内容会显示服务器当前 IP，登录 Cloudflare DNS 选项 查看之前设置的 1.1.1.1 已变为当前服务器的 IP。 六、设置 crontab 定时任务 # 编辑定时任务 crontab -e # 将以下内容添加到crontab中 # 无日志 */2 * * * * /root/cf-v4-ddns.sh \u003e/dev/null 2\u003e\u00261 如果需要日志文件，可将上述代码请替换成下述代码 */2 * * * * /root/cf-v4-ddns.sh \u003e\u003e /var/log/cf-ddns.log 2\u003e\u00261 ","date":"2019-05-25","objectID":"/posts/linux-server-using-cloudflare-to-build-ddns-service.html:0:0","series":null,"tags":["cloudflare","ddns"],"title":"Linux 服务器使用 Cloudflare 搭建 DDNS","uri":"/posts/linux-server-using-cloudflare-to-build-ddns-service.html"},{"categories":["Linux"],"content":"crontab 命令常见于 Unix 和类 Unix 的操作系统之中，用于设置周期性被执行的指令。 一、命令格式及其含义 #语义 * * * * * command ┃ ┃ ┃ ┃ ┃ ┃ ┃ ┃ ┃ ┃ ┃ ┗━需要执行的命令或脚本的路径 ┃ ┃ ┃ ┃ ┗━━━星期 取值范围 0-6 ┃ ┃ ┃ ┗━━━━月份 取值范围 1-12 ┃ ┃ ┗━━━━━ 日 取值范围 1-31 ┃ ┗━━━━━━ 小时 取值范围 0-23 ┗━━━━━━━━ 分钟 取值范围 0-59 #编辑crontab任务，写入后保存退出生效 crontab -e #列出已经存在的crontab任务 crontab -l 二、Demo #每天7:00重启 0 7 * * * reboot #每周六凌晨4:00重启 0 4 * * 6 reboot #每周六凌晨4:05执行脚本 5 4 * * 6 /root/clearLog.sh #每周六凌晨4:15执行 15 4 * * 6 /root/clearLog.sh #每天8:40执行 40 8 * * * /root/clearLog.sh #每周一到周五的11:41开始，每隔10分钟执行一次 41,51 11 * * 1-5 /root/clearLog.sh 1-59/10 12-23 * * 1-5 /root/clearLog.sh #在每天的10:31开始，每隔2小时重复一次 31 10-23/2 * * * /root/clearLog.sh #每天23:50执行 50 23 * * * /root/clearLog.sh #每天10:00、16:00执行 0 10,16 * * * /root/clearLog.sh REF Linux 计划任务 Crontab 实例详解：https://www.osyunwei.com/archives/5039.html ","date":"2019-05-20","objectID":"/posts/linux-using-crontab-to-execute-timed-tasks.html:0:0","series":null,"tags":["crontab"],"title":"Linux 使用 crontab 执行定时任务","uri":"/posts/linux-using-crontab-to-execute-timed-tasks.html"},{"categories":["Linux"],"content":"目前端口扫描可以通过 web 版本的端口扫描进行简单的测试，一次性扫描的端口数量一般也会有限制。通常情况下，如果仅仅是扫描一些常用端口，如上图所示的网页版端口扫描可以满足。如果想折腾一下，或者是对自己的主机进行安全检查之类的，还是使用 Linux 系统下的 nmap 工具进行扫描会比较方便一点。 一、安装 nmap 使用 yum 安装 nmap #更新一下源 yum update -y #安装 yum install nmap -y 二、nmap 命令 #扫描本机开放的端口「127.0.0.1 一般指代本机」 nmap 127.0.0.1 -p 1-65535 先咕咕咕… 日后再补批量扫 IP 段并将结果写入日志 ","date":"2019-05-19","objectID":"/posts/using-nmap-for-port-scanning-on-linux-systems.html:0:0","series":null,"tags":["nmap","端口扫描"],"title":"Linux 使用 nmap 进行端口扫描","uri":"/posts/using-nmap-for-port-scanning-on-linux-systems.html"},{"categories":["闲聊"],"content":"关于内容和工具。 大概在半年前，尝试并且使用了一些笔记软件，于是逐渐陷入寻找的沼泽里，一发不可收拾，开始体验各种笔记软件，这似乎是陷入了一个不应该存在的怪圈：找笔记APP，体验，找下一个笔记APP。而我们本心不该是这样的，本来是想找个称手的兵器打仗，最后却变成了武器商店的店长。 近期逐渐意识到了，应该逐渐回归初心，想到什么内容，就找合适的地方马上开始写，内容远比载体重要的多。 不再去把大量的时间花在寻找好工具上了，专注于内容产出，提升自己，显然更加重要。 ","date":"2019-05-18","objectID":"/posts/about.html:0:0","series":null,"tags":[],"title":"关于博客","uri":"/posts/about.html"},{"categories":["闲聊"],"content":"关于内容和工具。 大概在半年前，尝试并且使用了一些笔记软件，于是逐渐陷入寻找的沼泽里，一发不可收拾，开始体验各种笔记软件，这似乎是陷入了一个不应该存在的怪圈：找笔记APP，体验，找下一个笔记APP。而我们本心不该是这样的，本来是想找个称手的兵器打仗，最后却变成了武器商店的店长。 近期逐渐意识到了，应该逐渐回归初心，想到什么内容，就找合适的地方马上开始写，内容远比载体重要的多。 不再去把大量的时间花在寻找好工具上了，专注于内容产出，提升自己，显然更加重要。 ","date":"2019-05-18","objectID":"/posts/about.html:0:0","series":null,"tags":[],"title":"关于博客","uri":"/posts/about.html"}]