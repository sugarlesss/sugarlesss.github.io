[{"categories":["折腾","Python"],"content":"长时间的电话 / 会议录音 / 演讲视频之类的各种音视频媒体文件的信息，如果有长期存储的需求的话，你可能会因为体积积太大，不方便进行信息检索而头疼。但如果转成文字，润色后整理成文字稿，无论是从存储体积或是信息检索方面，都会比直接存储音视频媒体更方便些。 尝遍了市面上各种音频转文字的野鸡产品后，我最终还是选择用大厂提供的 API ，to B 的产品在价格和可定制性上肯定比 to C 的产品更有优势。 关于如何将视频转音频，手段很多，比如使用 FFmpeg 可以将视频转为音频： ffmpeg -i input.mp4 -vn -acodec copy audio.mp3 一、音频转文字究竟有多贵？ 参考当前的大型互联网厂商提供的音频转文字服务定价，讯飞最贵，百度云虽然最便宜，但有起购门槛，腾讯云的价格最合适，而且最关键的是腾讯云每月都有附赠一定量的额度，可以无限白嫖。 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:0:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"1.1、讯飞 https://www.xfyun.cn/services/lfasr https://www.xfyun.cn/services/lfasr#anchor4503211 有效期一年 套餐一（ 20 小时） 168 元 / 单价 8.4 套餐二（ 200 小时） 980 元 / 单价 4.9 套餐三（ 1000 小时） 3900 元 / 单价 3.9 套餐四（ 3000 小时） 10500 元 / 单价 3.5 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:1:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"1.2、百度云 https://ai.baidu.com/tech/speech/aasr https://ai.baidu.com/ai-doc/SPEECH/Jk38lxn2j#按小时包预付费-1 按小时包预付费（有效期一年） 套餐一（ 1000 小时） 1200 元 / 单价 1.2 套餐二（ 10000 小时） 9000 元 / 单价 0.9 套餐三（ 100000 小时） 70000 元 / 单价 0.7 套餐四（ 500000 小时） 300000 元 / 单价 0.6 按调用时长后付费 每小时 2 元，系统按用户实际使用，每小时出账单实时扣费，账户内需保留足量余额。 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:2:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"1.3、腾讯云 腾讯云查看免费配额：https://console.cloud.tencent.com/asr/resourcebundle 腾讯云语音识别资源包购买地址：https://buy.cloud.tencent.com/asr 录音文件识别（五小时内出结果） 套餐一（ 60 小时） 90 元 / 单价 1.5 套餐二（ 1000 小时） 1200 元 / 单价 1.2 套餐三（ 10000 小时） 10000 元 / 单价 1 套餐四（ 100000 小时） 80000 元 / 单价 0.8 套餐五（ 300000 小时） 210000 元 / 单价 0.7 支持中文普通话、英语、粤语、日语、泰语。对时长5小时以内的录音文件进行识别，异步返回识别全部结果。 支持语音 URL 和本地语音文件两种请求方式。 语音 URL 的音频时长不能长于5小时，文件大小不超过512MB。 本地语音文件不能大于5MB。 提交录音文件识别请求后，在5小时内完成识别（半小时内发送超过1000小时录音或者2万条识别任务的除外），识别结果在服务端可保存7天 支持回调或轮询的方式获取结果 录音文件识别极速版（准实时） 套餐一（ 30 小时） 72 元 / 单价 2.3 套餐二（ 1000 小时） 1500 元 / 单价 1.5 套餐三（ 10000 小时） 12000 元 / 单价 1.2 套餐四（ 100000 小时） 110000 元 / 单价 1.1 套餐五（ 300000 小时） 300000 元 / 单价 1 仅支持中文普通话，使用者通过 HTTPS POST 方式上传一段音频并在极短时间内同步返回识别结果，可满足音视频字幕、准实时质检等场景下对语音文件识别时效性的要求。 支持100MB以内音频文件的识别 二、对接腾讯云 在使用腾讯云 API 之前，你需要先获取三个必要的参数。 在腾讯云控制台账号信息页面查看账号 APPID：https://console.cloud.tencent.com/developer 访问管理页面获取 SecretID 和 SecretKey：https://console.cloud.tencent.com/cam/capi ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:3:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"2.1、录音文件识别极速版 极速版演示，目前腾讯云给的免费配额是每月 5 小时。 极速版仅支持中文普通话，通过 HTTPS POST 方式上传一段音频并在极短时间内同步返回识别结果，可满足音视频字幕、准实时质检等场景下对语音文件识别时效性的要求。支持100MB以内音频文件的识别。 1）将真实的值填入下方源代码的 APPID、SECRET_ID、SECRET_KEY 这三个参数中； 2）将源代码中的 audio 参数修改为本地音频文件的相对或绝对路径； 3）run it。 # 录音文件识别极速版 v2版本 # API 文档：https://cloud.tencent.com/document/api/1093/52097 # 签名生成：https://cloud.tencent.com/document/api/1093/52097#sign import json import requests import time import hashlib import hmac import base64 # 调用者身份 class Credential: def __init__(self, app_id, secret_id, secret_key): self.app_id = app_id self.secret_id = secret_id self.secret_key = secret_key # 服务端信息 class Server: def __init__(self, protocol, host, port, uri): self.protocol = protocol self.host = host self.port = port self.uri = uri # 打印日志 def show(title=\"\", content=object, level=\"INFO\"): print(\"[\" + level + \"] \" + title) print(content) print() # 获取时间戳 def getTimestamp(): t = time.time() # return t # 原始时间数据 return int(t) # 秒级时间戳 # return int(round(t * 1000)) # 毫秒级时间戳 # return int(round(t * 1000000)) # 微秒级时间戳 # 字典序排序 def format_sign_string(param): param = sorted(param.items(), key=lambda d: d[0]) signstr = f\"POST{server.host + server.uri + credential.app_id}\" for t in param: if 'appid' in t: signstr += str(t[1]) break signstr += \"?\" for x in param: tmp = x if 'appid' in x: continue for t in tmp: signstr += str(t) signstr += \"=\" signstr = signstr[:-1] signstr += \"\u0026\" signstr = signstr[:-1] return signstr # 签名 def sign(signstr, secret_key): hmacstr = hmac.new(secret_key.encode('utf-8'), signstr.encode('utf-8'), hashlib.sha1).digest() s = base64.b64encode(hmacstr) s = s.decode('utf-8') return s # 调用者信息对象初始化 # 在腾讯云控制台账号信息页面查看账号 APPID：https://console.cloud.tencent.com/developer # 访问管理页面获取 SecretID 和 SecretKey：https://console.cloud.tencent.com/cam/capi APPID = \"\" SECRET_ID = \"\" SECRET_KEY = \"\" credential = Credential(APPID, SECRET_ID, SECRET_KEY) # 服务端信息对象初始化 PROTOCOL = \"https://\" HOST = \"asr.cloud.tencent.com\" PORT = \"\" URI = \"/asr/flash/v1/\" server = Server(PROTOCOL, HOST, PORT, URI) # 构造参数 params = { # 用户在腾讯云注册账号 AppId 对应的 SecretId \"secretid\": f\"{credential.secret_id}\", # 引擎模型类型。8k_zh：8k 中文普通话通用；16k_zh：16k 中文普通话通用；16k_zh_video：16k 音视频领域。 \"engine_type\": \"16k_zh\", # 音频格式。支持 wav、pcm、ogg-opus、speex、silk、mp3、m4a、aac。 \"voice_format\": \"mp3\", # 当前 UNIX 时间戳，如果与当前时间相差超过3分钟，会报签名失败错误。 \"timestamp\": getTimestamp(), } # 是否开启说话人分离（目前支持中文普通话引擎），默认为0，0：不开启，1：开启。 # params[\"speaker_diarization\"] = 0; # 是否过滤脏词（目前支持中文普通话引擎），默认为0。0：不过滤脏词；1：过滤脏词；2：将脏词替换为 *。 # params[\"filter_dirty\"] = 0; # 是否过滤语气词（目前支持中文普通话引擎），默认为0。0：不过滤语气词；1：部分过滤；2：严格过滤。 # params[\"filter_modal\"] = 0; # 是否过滤标点符号（目前支持中文普通话引擎），默认为0。0：不过滤，1：过滤句末标点，2：过滤所有标点。 # params[\"filter_punc\"] = 0; # 是否进行阿拉伯数字智能转换，默认为1。0：全部转为中文数字；1：根据场景智能转换为阿拉伯数字。 params[\"convert_num_mode\"] = 1; # 是否显示词级别时间戳，默认为0。0：不显示；1：显示，不包含标点时间戳，2：显示，包含标点时间戳。 # params[\"word_info\"] = 0; # 是否只识别首个声道，默认为1。0：识别所有声道；1：识别首个声道。 # params[\"first_channel_only\"] = 1; show(\"生成 params\", params) # 获取 signature signstr = format_sign_string(params) signature = sign(signstr, SECRET_KEY) show(\"生成 signstr\", signstr) show(\"生成 signature\", signature) # 计算 URL URL = server.protocol + signstr[4::] show(\"计算 URL\", URL) # 构造 header headers = { \"Host\": \"asr.cloud.tencent.com\", \"Authorization\": f\"{signature}\", # \"Content-Type\": \"application/octet-stream\", # \"Content-Length\": \"请求长度，此处对应语音数据字节数，单位：字节\" } show(\"生成 headers\", headers) # 音频路径 audio = \"./外卖佣金到底有多高.mp3\" with open(audio, 'rb') as f: # 读取音频数据 data = f.read() # 调用腾讯云语音识别 API e = requests.post(URL, headers=headers, data=data) responese = json.loads(e.text) code = responese[\"code\"] if code != 0: show(\"识别失败\", responese) else: show(\"识别成功\", responese) # 一个channl_result对应一个声道的识别结果 # 大多数音频是单声道，对应一个channl_result for channl_result in responese[\"flash_result\"]: channel_id = channl_result['channel_id'] text = channl_result['text'] show(f\"channel_id: {channel_id}\", text) 输出 [INFO] 生成 params {'secretid': 'xxxxxxxxxxxxxxxxxxx', 'engine_type': '16k_zh', 'voice_format': 'mp3', 'timestamp': xxxxxxxxx, 'convert_num","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:4:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"2.2、录音文件识别 录音文件识别演示，目前腾讯云给的免费配额是每月 10 小时。 录音文件识别请求 API 文档：https://cloud.tencent.com/document/api/1093/37823 录音文件识别结果查询 API 文档：https://cloud.tencent.com/document/api/1093/37822 腾讯云 API 3.0 提供了配套的开发工具集 SDK：https://cloud.tencent.com/document/api/1093/37823#SDK ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:5:0","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"2.2.1、安装 Python SDK 在使用录音文件识别时，需要通过 pip 方式安装腾讯云提供的 Python 版本 SDK: pip install --upgrade tencentcloud-sdk-python # 中国大陆地区的用户可以使用国内镜像源提高下载速度 pip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python。 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:5:1","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"2.2.2、上传本地录音文件识别 腾讯云提供的 API Explorer 可以很方便的生成代码和参数：https://console.cloud.tencent.com/api/explorer?Product=asr\u0026Version=2019-06-14\u0026Action=CreateRecTask\u0026SignVersion= 下面我提供一个自己的 demo ，运行前记得替换三个参数为真实值。其中 audio 是本地录音文件相对路径或绝对路径。 import json import base64 from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.asr.v20190614 import asr_client, models SecretId = \"xxxxxxxxxxxxxxxxxxxxxxxxx\" SecretKey = \"xxxxxxxxxxxxxxxxxxxxxxxxx\" audio = \"./audio.mp3\" # 将录音转为字符串 Data = \"\" with open(audio, 'rb') as f: # 读取音频数据 Data = f.read() Data = base64.b64encode(Data) Data = Data.decode() try: cred = credential.Credential(SecretId, SecretKey) httpProfile = HttpProfile() httpProfile.endpoint = \"asr.tencentcloudapi.com\" clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = asr_client.AsrClient(cred, \"\", clientProfile) req = models.CreateRecTaskRequest() params = { ''' 引擎模型类型。 这里的 k 指的是采样率 电话场景： • 8k_en：电话8k英语； • 8k_zh：电话8k中文普通话通用； 非电话场景： • 16k_zh：16k 中文普通话通用； • 16k_zh_video：16k 音视频领域； • 16k_en：16k 英语； • 16k_ca：16k 粤语； • 16k_ja：16k 日语； • 16k_zh_edu 中文教育； • 16k_en_edu 英文教育； • 16k_zh_medical 医疗； • 16k_th 泰语； ''' \"EngineModelType\": \"16k_zh\", ''' 识别声道数。注意：录音识别会自动将音频转码为填写的识别声道数 1：单声道； 2：双声道（仅支持 8k_zh 引擎模）。 ''' \"ChannelNum\": 1, ''' 识别结果返回形式。 0： 识别结果文本(含分段时间戳)； 1：词级别粒度的详细识别结果(不含标点，含语速值)； 2：词级别粒度的详细识别结果（包含标点、语速值） ''' \"ResTextFormat\": 0, ''' 语音数据来源。 0：语音 URL； 1：语音数据（post body）。 ''' \"SourceType\": 1, ''' 语音数据，当SourceType 值为1时必须填写，为0可不写。 要base64编码(采用python语言时注意读取文件应该为string而不是byte，以byte格式读取后要decode()。 编码后的数据不可带有回车换行符)。音频数据要小于5MB。 ''' \"Data\": Data } req.from_json_string(json.dumps(params)) resp = client.CreateRecTask(req) print(resp.to_json_string()) except TencentCloudSDKException as err: print(err) 输出 { \"Data\":{ \"TaskId\":1234567890 }, \"RequestId\":\"f1234567-89a4-1234-12d3-d56bdd9aac1a\" } 请求成功后，返回的 JSON 中 Data -\u003e TaskId 就是我们此次上传任务的 ID ，需要拿这个 ID 去轮训另一个接口，查询是否成功。 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:5:2","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾","Python"],"content":"2.2.3、查询录音文件识别结果 腾讯云提供的 API Explorer 可以很方便的生成代码和参数：https://console.cloud.tencent.com/api/explorer?Product=asr\u0026Version=2019-06-14\u0026Action=DescribeTaskStatus\u0026SignVersion= 用 TaskID 查询识别结果。 import json from tencentcloud.common import credential from tencentcloud.common.profile.client_profile import ClientProfile from tencentcloud.common.profile.http_profile import HttpProfile from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException from tencentcloud.asr.v20190614 import asr_client, models SecretId = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" SecretKey = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" TaskId = 1234567890 try: cred = credential.Credential(SecretId, SecretKey) httpProfile = HttpProfile() httpProfile.endpoint = \"asr.tencentcloudapi.com\" clientProfile = ClientProfile() clientProfile.httpProfile = httpProfile client = asr_client.AsrClient(cred, \"\", clientProfile) req = models.DescribeTaskStatusRequest() params = { \"TaskId\": TaskId } req.from_json_string(json.dumps(params)) resp = client.DescribeTaskStatus(req) print(resp.to_json_string()) except TencentCloudSDKException as err: print(err) 输出： { \"Data\":{ \"TaskId\":1234567890, \"Status\":2, \"StatusStr\":\"success\", \"Result\":\"[0:0.000,1:0.320] 识别结果。\\n[1:0.320,2:0.360] 识别结果。\\n[2:0.360,3:0.380] 识别结果。\\n[3:0.380,4:0.400] 识别结果。\\n[4:0.400,5:0.420] 识别结果。\\n\", \"ErrorMsg\":\"\", \"ResultDetail\":null }, \"RequestId\":\"12345678-1234-1234-1234-b11234567890\" } REF 长时间的会议录音如何快速转化成文字：https://www.zhihu.com/question/21552953 如何用ffmpeg从mkv视频文件中提取音频？：https://www.zhihu.com/question/420452079 ","date":"2021-07-16","objectID":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/:5:3","series":null,"tags":["音频处理","语音转文字","FFmpeg"],"title":"关于音视频文案提取","uri":"/2021/07/16/%E5%85%B3%E4%BA%8E%E9%9F%B3%E8%A7%86%E9%A2%91%E6%96%87%E6%A1%88%E6%8F%90%E5%8F%96/"},{"categories":["折腾"],"content":"尽量简化不必要的流程，保持最简洁的操作，只需一条命令即可完成 hugo 静态 html 渲染、提交到 github 等一系列操作。 本操作只在 windows 平台下实践过，其他平台可自行尝试。 先安装 git bash：https://git-scm.com/downloads 确定hugo 博客的根目录：public 的上级目录就是你博客的根目录，我的根目录是 D:\\blog\\src D:\\BLOG\\SRC │ config.toml ├─archetypes ├─assets ├─content ├─data ├─layouts ├─public ├─resources ├─static └─themes 将 git 仓库（xxx.github.io）的 .git 文件夹和 CNAME 文件复制一份到博客根目录的 public 文件夹中，使 public 目录成为一个仓库。 配置 git alias 别名，这里我将 git blog 这个命令作为以后日常一键部署的命令，注意，此处的博客根目录不要填写错误。 git config --global alias.blog '!cd D:\\\\blog\\\\src;hugo;cd D:\\\\blog\\\\src\\\\public;git add .;git commit -m 'update';git push' 以后写好文章之后，就可以在任意目录执行这个命令： git blog 即可一键渲染、并提交推送到 github 上了，配合 git pages 使用更佳。 ","date":"2021-07-14","objectID":"/2021/07/14/hugo-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2/:0:0","series":null,"tags":["hugo"],"title":"hugo 一键部署","uri":"/2021/07/14/hugo-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2/"},{"categories":[],"content":"WizeTree 是一款 Windows 平台下的磁盘空间分析器。 通过可视化（图形化、树形化）的布局，你可以直观地看到在你硬盘上大的文件和文件夹。 内置文件管理器，支持查看文件树、文件夹大小排序、文件类型分析 wiztree 使用 NTFS 文件系统的 MFT 进行文件分析 (与著名的软件 everything 原理相同) 比 spacesniffer 的速度快数十倍，几秒钟就能完成全盘文件大小分析。 官网：https://www.diskanalyzer.com/ 预览图 ","date":"2021-07-14","objectID":"/2021/07/14/wiztree-%E6%9C%80%E5%BF%AB%E7%9A%84%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E5%88%86%E6%9E%90%E5%99%A8/:0:0","series":null,"tags":["软件"],"title":"WizTree - 最快的磁盘空间分析器","uri":"/2021/07/14/wiztree-%E6%9C%80%E5%BF%AB%E7%9A%84%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E5%88%86%E6%9E%90%E5%99%A8/"},{"categories":null,"content":"一个 Php 转 Java 的后端工程师。 Email jaded@foxmail.com ","date":"2021-07-13","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"CrownDaisy said, Less is more. xiaoz said, 生命不息，吾将折腾不止。 ","date":"2021-07-13","objectID":"/friends/:0:0","series":null,"tags":null,"title":"友链 / 朋友们","uri":"/friends/"},{"categories":["折腾"],"content":"有些网页上的视频是分成多个 ts 片段的，无法被 chrome的 各种嗅探器插件捕获，但通过 F12 开发工具监测网络（Network）时，在过滤器中输入 m3u8，可以发现一个独立的 m3u8 文件，这个文件就是记录了所有 ts 文件片段的一个播放列表。 文件内容大致像这样： 如果没有发现独立的 m3u8 文件，有可能每一个 ts 文件的地址中也是含有这个 m3u8 文件的名称的。把这个 m3u8 文件的完整地址截取出来。针对这种情况，我就不具体举例了，因为我还没遇到过。 到 header tab 里，复制一下这个 m3u8 文件的完整 url ，画红线的这部分就是。 假设这个地址是：https://xxx.abc.com/xxx/a.m3u8 可以使用ffmpeg（FFmpeg）命令下载合并输出为一个视频文件 ffmpeg -i https://xxx.abc.com/xxx/a.m3u8 -c copy output.mp4 ","date":"2021-07-13","objectID":"/2021/07/13/%E5%A6%82%E4%BD%95%E4%B8%8B%E8%BD%BD-ts-%E6%B5%81%E5%AA%92%E4%BD%93%E8%A7%86%E9%A2%91/:0:0","series":null,"tags":["流媒体","FFmpeg"],"title":"如何下载 ts 流媒体视频","uri":"/2021/07/13/%E5%A6%82%E4%BD%95%E4%B8%8B%E8%BD%BD-ts-%E6%B5%81%E5%AA%92%E4%BD%93%E8%A7%86%E9%A2%91/"},{"categories":["Python"],"content":"问题：有些批量下载的视频会带固定前缀，在视频播放器的播放列表里显示非常不友好。 场景：在 “D:\\纪录片\\中国通史” 路径下有 100 集视频文件，每个文件都带有固定前缀 “www.baidu.com 出品 微信公众号 xxx” 字样。 源码 RemoveFixedPrefix.py ，因为源码使用了 python fstring 的特性，需要在 python \u003e= 3.6 的版本中使用。 # coding=utf-8 import os # 目标路径 srcPath = \"D:\\纪录片\\中国通史\" fileList = os.listdir(srcPath) # 固定前缀 FixedPrefix = \"www.baidu.com出品 微信公众号xxx\" for fileName in fileList: oldFilePath = f\"{srcPath}/{fileName}\" # 跳过目录 if os.path.isdir(oldFilePath): continue newFileName = fileName.replace(FixedPrefix, \"\") newFilePath = f\"{srcPath}/{newFileName}\" try: os.rename(oldFilePath, newFilePath) except Exception as e: print(e) print(newFileName) ","date":"2021-06-21","objectID":"/2021/06/21/python-%E6%89%B9%E9%87%8F%E5%8E%BB%E9%99%A4%E6%96%87%E4%BB%B6%E5%9B%BA%E5%AE%9A%E5%89%8D%E7%BC%80/:0:0","series":null,"tags":[],"title":"Python 批量去除文件固定前缀","uri":"/2021/06/21/python-%E6%89%B9%E9%87%8F%E5%8E%BB%E9%99%A4%E6%96%87%E4%BB%B6%E5%9B%BA%E5%AE%9A%E5%89%8D%E7%BC%80/"},{"categories":["PHP"],"content":"结论推导 一、结论 # 最方便 echo strtotime('23:59:59') - time(); #最快 echo 86400 - (time() + 28800) % 86400; 二、推导过程 用 86400 减去今天已经过去了多少秒，即可求得今天还剩多少秒。 86400=24*3600，即一天的总秒数。 28800=8*3600，即 8 个小时的总秒数。 当前时间戳取模 86400 并不是今天已经过去了多少秒，因为时间戳起始时间并不是 0 点，而是 8 点整。所以，如果当前是早上 8 点整，取模 86400 后会等于 0，与我们的本意不符（求今天已经过去了多少秒）。 因此，要用当前时间戳加上 8 个小时的总秒数后再取模 86400，即可求得今天过去了多少秒。 结论是由下面的算法简化后得到的： 86400 - (time() + 8 * 3600) % 86400 三、REF https://segmentfault.com/a/1190000019844608 ","date":"2021-01-12","objectID":"/2021/01/12/php-%E8%AE%A1%E7%AE%97%E5%BD%93%E5%A4%A9%E5%89%A9%E4%BD%99%E7%A7%92%E6%95%B0%E6%9C%80%E6%96%B9%E4%BE%BF%E5%92%8C%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E6%B3%95/:0:0","series":null,"tags":[],"title":"PHP计算当天剩余秒数最方便和最快的方法","uri":"/2021/01/12/php-%E8%AE%A1%E7%AE%97%E5%BD%93%E5%A4%A9%E5%89%A9%E4%BD%99%E7%A7%92%E6%95%B0%E6%9C%80%E6%96%B9%E4%BE%BF%E5%92%8C%E6%9C%80%E5%BF%AB%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["python"],"content":"此前写过一篇基于 BeautifulSoup 库开发的 demo，这次用 xpath 写。 源码 # -*- coding:utf-8 -*- # import requests from lxml import etree def get_headers(): headers = {} headers[\"content-type\"] = \"text/html;\" headers[ \"user-agent\"] = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36\" headers[\"host\"] = \"top.baidu.com\" return headers url = 'http://top.baidu.com/buzz?b=1' response = requests.get(url, headers=get_headers()) response.encoding = 'gbk' html = response.text if response.status_code != 200: print(f'返回状态码：{response.status_code}') exit(0) # 调用HTML类进行初始化 html = etree.HTML(html) # 提取该页面所有标题 result_all = html.xpath('//*[@id=\"main\"]/div[2]/div/table/tr/td[2]/a[1]') # 打印所有提取出的新闻标题 for v in result_all: print(v.text) 在开发时遇到一个有意思的坑，如果你测试时使用浏览器复制的热点标题的 title xpath，你会发现获取不到标题。你只需要把 tbody 标签去掉，就可以正常获取到标题了。 输出 31省新增本土病例85例:河北82例 佩洛西:众议院将第二次弹劾特朗普 河北新增49例本地无症状感染者 金正恩被推举为朝鲜劳动党总书记 北京新增1例确诊 4例无症状感染者 百度宣布组建智能汽车公司 美发生连环枪击案 一留学生身亡 日本发现新型变异新冠病毒 死刑!曾春亮案一审宣判 青藏高原云南等地降温雨雪来了 拼多多回应员工匿名发帖被辞退 石家庄新增确诊曾去过武汉汉正街 吉林新增4例本土无症状:2对夫妻 袁咏仪 送包给我是张智霖的福分 宋小女:这个结局也挺好 2020年CPI较上年上涨2.5% 全棉时代道歉疑似打广告 韩媒:韩军发现朝鲜举行阅兵式迹象 国会骚乱后特朗普没联系过彭斯 天津处罚过马路的低头族 被放生秃鹫赖警局每天伙食费150 比特币暴跌超10% 20岁小姐姐当汽车兵驰骋川藏线 北京乘出租车网约车需扫健康宝 网上买菜莫名被开通美团月付 自低风险区返乡要检测?多地出通知 车厘子价格腰斩 山东长岛海边现冰冻奇观似鸳鸯锅 武汉向石家庄捐赠50吨蔬菜 康辉说和21岁最大差别是脸的宽度 REF https://blog.csdn.net/qq_36523839/article/details/79992002 ","date":"2021-01-11","objectID":"/2021/01/11/python-%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E5%AE%9E%E6%97%B6%E7%83%AD%E7%82%B9/:0:0","series":null,"tags":[],"title":"Python 爬取百度实时热点","uri":"/2021/01/11/python-%E7%88%AC%E5%8F%96%E7%99%BE%E5%BA%A6%E5%AE%9E%E6%97%B6%E7%83%AD%E7%82%B9/"},{"categories":["Linux"],"content":"在高并发短连接的 TCP 服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量 socket 处于 TIME_WAIT 状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个 0~65535 的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。 在这个场景中，短连接表示 “业务处理 + 传输数据的时间 远远小于 TIMEWAIT 超时的时间” 的连接。Linux 默认的 TIME_WAIT 时长一般是 60 秒。 查看默认 timewait 时长 cat /proc/sys/net/ipv4/tcp_fin_timeout 查看连接状态统计 netstat -an | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}' 优化内核参数 vim /etc/sysctl.conf #追加内容 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_fin_timeout = 30 释义 开启 SYN Cookies。当出现 SYN 等待队列溢出时，启用 cookies 来处理，可防范少量 SYN 攻击，默认为 0，表示关闭。 net.ipv4.tcp_syncookies = 1 开启重用。允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，默认为 0，表示关闭。 net.ipv4.tcp_tw_reuse = 1 开启 TCP 连接中 TIME-WAIT sockets 的快速回收，默认为 0，表示关闭。 net.ipv4.tcp_tw_recycle = 1 修改系統默认的 TIMEOUT 时间（FIN_WAIT_2 状态的时长） net.ipv4.tcp_fin_timeout REF https://www.cnblogs.com/apanly/p/12431902.html https://zhuanlan.zhihu.com/p/79507132 ","date":"2021-01-03","objectID":"/2021/01/03/linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%87%BA%E7%8E%B0%E5%A4%A7%E9%87%8F-time_wait-%E7%8A%B6%E6%80%81%E7%9A%84%E8%BF%9E%E6%8E%A5/:0:0","series":null,"tags":[],"title":"Linux服务器出现大量TIME_WAIT状态的连接","uri":"/2021/01/03/linux-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%87%BA%E7%8E%B0%E5%A4%A7%E9%87%8F-time_wait-%E7%8A%B6%E6%80%81%E7%9A%84%E8%BF%9E%E6%8E%A5/"},{"categories":["PHP"],"content":"无限级分类树生成可以使用递归或引用实现，但递归效率太慢，使用引用特性实现会是一个更好的方式。 源码 public function test() { // 初始数据 $items = array( array('id' =\u003e 1, 'pid' =\u003e 0, 'name' =\u003e '福建省'), array('id' =\u003e 2, 'pid' =\u003e 0, 'name' =\u003e '四川省'), array('id' =\u003e 3, 'pid' =\u003e 1, 'name' =\u003e '福州市'), array('id' =\u003e 4, 'pid' =\u003e 2, 'name' =\u003e '成都市'), array('id' =\u003e 5, 'pid' =\u003e 2, 'name' =\u003e '乐山市'), array('id' =\u003e 6, 'pid' =\u003e 4, 'name' =\u003e '成华区'), array('id' =\u003e 7, 'pid' =\u003e 4, 'name' =\u003e '龙泉驿区'), array('id' =\u003e 8, 'pid' =\u003e 6, 'name' =\u003e '崔家店路'), array('id' =\u003e 9, 'pid' =\u003e 7, 'name' =\u003e '龙都南路'), array('id' =\u003e 10, 'pid' =\u003e 8, 'name' =\u003e 'A店铺'), array('id' =\u003e 11, 'pid' =\u003e 9, 'name' =\u003e 'B店铺'), array('id' =\u003e 12, 'pid' =\u003e 8, 'name' =\u003e 'C店铺'), array('id' =\u003e 13, 'pid' =\u003e 1, 'name' =\u003e '泉州市'), array('id' =\u003e 14, 'pid' =\u003e 13, 'name' =\u003e '南安县'), array('id' =\u003e 15, 'pid' =\u003e 13, 'name' =\u003e '惠安县'), array('id' =\u003e 16, 'pid' =\u003e 14, 'name' =\u003e 'A镇'), array('id' =\u003e 17, 'pid' =\u003e 14, 'name' =\u003e 'B镇'), array('id' =\u003e 18, 'pid' =\u003e 16, 'name' =\u003e 'A村'), array('id' =\u003e 19, 'pid' =\u003e 16, 'name' =\u003e 'B村'), ); // 根据初始数据，生成一个以 id 为 key/下标 的数组，方便根据 pid 判断是否存在父级元素。 $items = array_column($items,null,'id'); //使用 php 的 \u0026 引用特性，遍历一次循环即可生成无限级分类树。（其他高级语言中也有类似的特性，诸如 C++ 的指针和 JAVA 的引用） $tree = []; foreach ($items as $item) { $id = $item['id']; $pid = $item['pid']; if (isset($items[$pid])) $items[$pid]['children'][] = \u0026$items[$id]; else $tree[] = \u0026$items[$id]; } $this-\u003esuccess('ok',$tree); } 输出 { \"code\": 1, \"msg\": \"ok\", \"data\": [ { \"id\": 1, \"pid\": 0, \"name\": \"福建省\", \"children\": [ { \"id\": 3, \"pid\": 1, \"name\": \"福州市\" }, { \"id\": 13, \"pid\": 1, \"name\": \"泉州市\", \"children\": [ { \"id\": 14, \"pid\": 13, \"name\": \"南安县\", \"children\": [ { \"id\": 16, \"pid\": 14, \"name\": \"A镇\", \"children\": [ { \"id\": 18, \"pid\": 16, \"name\": \"A村\" }, { \"id\": 19, \"pid\": 16, \"name\": \"B村\" } ] }, { \"id\": 17, \"pid\": 14, \"name\": \"B镇\" } ] }, { \"id\": 15, \"pid\": 13, \"name\": \"惠安县\" } ] } ] }, { \"id\": 2, \"pid\": 0, \"name\": \"四川省\", \"children\": [ { \"id\": 4, \"pid\": 2, \"name\": \"成都市\", \"children\": [ { \"id\": 6, \"pid\": 4, \"name\": \"成华区\", \"children\": [ { \"id\": 8, \"pid\": 6, \"name\": \"崔家店路\", \"children\": [ { \"id\": 10, \"pid\": 8, \"name\": \"A店铺\" }, { \"id\": 12, \"pid\": 8, \"name\": \"C店铺\" } ] } ] }, { \"id\": 7, \"pid\": 4, \"name\": \"龙泉驿区\", \"children\": [ { \"id\": 9, \"pid\": 7, \"name\": \"龙都南路\", \"children\": [ { \"id\": 11, \"pid\": 9, \"name\": \"B店铺\" } ] } ] } ] }, { \"id\": 5, \"pid\": 2, \"name\": \"乐山市\" } ] } ] ","date":"2020-12-16","objectID":"/2020/12/16/php-%E5%9F%BA%E4%BA%8E%E5%BC%95%E7%94%A8%E7%89%B9%E6%80%A7%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%97%A0%E9%99%90%E7%BA%A7%E5%88%86%E7%B1%BB%E6%A0%91/:0:0","series":null,"tags":[],"title":"PHP基于引用特性实现的无限级分类树","uri":"/2020/12/16/php-%E5%9F%BA%E4%BA%8E%E5%BC%95%E7%94%A8%E7%89%B9%E6%80%A7%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%97%A0%E9%99%90%E7%BA%A7%E5%88%86%E7%B1%BB%E6%A0%91/"},{"categories":["Linux"],"content":"Systemd 服务是一种以 .service 结尾的单元（unit）配置文件，用于控制由 Systemd 控制或监视的进程。简单说，用于后台以守护精灵（daemon）的形式运行程序。Systemd 广泛应用于新版本的 RHEL、SUSE Linux Enterprise、CentOS、Fedora 和 openSUSE 中，用于替代旧有的服务管理器 service。 一、如何创建一个服务？ 这里假设你已经自行编译安装好了 nginx，下面我们来创建一个 nginx.service 文件 vi /etc/systemd/system/nginx.service 内容如下： [Unit] Description=Nginx - high performance web server After=network.target [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop [Install] WantedBy=multi-user.target 重新加载服务配置文件，使创建的 nginx 服务生效： systemctl daemon-reload 这样我们就可以用 Systemd 的方式来管理 nginx 了，命令如下： #启动nginx systemctl start nginx #重载nginx systemctl reload nginx #停止nginx systemctl stop nginx #重启nginx systemctl restart nginx #如果需要开机启动 systemctl enable nginx #如果需要取消开机启动 systemctl disable nginx 二、关于 Systemd 服务 Systemd 服务的内容主要分为三个部分，控制单元（unit）的定义、服务（service）的定义、以及安装（install）的定义。 ","date":"2020-12-01","objectID":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/:0:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/"},{"categories":["Linux"],"content":"2.1、控制单元 unit 从上面的例子中我们看到 Unit 内容如下： [Unit] Description=Nginx - high performance web server After=network.target Description：代表整个单元的描述，可根据需要任意填写。 Before/After：指定启动顺序。 network.target 代表有网路，network-online.target 代表一个连通着的网络。 ","date":"2020-12-01","objectID":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/:1:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/"},{"categories":["Linux"],"content":"2.2、服务本体 service [Service] Type=forking ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/usr/local/nginx/sbin/nginx -s stop Type: 服务的类型，各种类型的区别如下所示 simple：默认，这是最简单的服务类型。意思就是说启动的程序就是主体程序，这个程序要是退出那么一切皆休。 forking：标准 Unix Daemon 使用的启动方式。启动程序后会调用 fork () 函数，把必要的通信频道都设置好之后父进程退出，留下守护精灵的子进程。 oneshot：适用于那些被一次性执行的任务或者命令，它运行完成后便了无痕迹。因为这类服务运行完就没有任何痕迹，我们经常会需要使用 RemainAfterExit=yes。意思是说，即使没有进程存在，Systemd 也认为该服务启动成功了。同时只有这种类型支持多条命令，命令之间用；分割，如需换行可以用 \\。 dbus：这个程序启动时需要获取一块 DBus 空间，所以需要和 BusName= 一起用。只有它成功获得了 DBus 空间，依赖它的程序才会被启动。 ExecStart：在输入的命令是 start 时候执行的命令，这里的命令启动的程序必须使用绝对路径，比如你必须用 /sbin/arp 而不能简单的以环境变量直接使用 arp。 ExecStop：在输入的命令是 stop 时候执行的命令，要求同上。 ExecReload：这个不是必需，如果不写则你的 service 就不支持 restart 命令。ExecStart 和 ExecStop 是必须要有的。 ","date":"2020-12-01","objectID":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/:2:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/"},{"categories":["Linux"],"content":"2.3、安装部分 install [Install] WantedBy=multi-user.target WantedBy：运行级别 / 设置服务被谁装载，一般设置为 multi-user.target（从 Centos7 以后 bai 采用 target 概念来定义运行级别，multi-user.target 是第三级别） ","date":"2020-12-01","objectID":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/:3:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/"},{"categories":["Linux"],"content":"2.4、存放的位置 Systemd Service 位于 /etc/systemd/system（供系统管理员和用户使用），/usr/lib/systemd/system（供发行版打包者使用），我们一般使用前者即可。 3、总结 Systemd Service 是一种替代 /etc/init.d/ 下脚本的更好方式，它可以灵活的控制你什么时候要启动服务，一般情况下也不会造成系统无法启动进入紧急模式。所以如果想设置一些开机启动的东西，可以试着写 Systemd Service。当然了，前提是你使用的 Linux 发行版是支持它的才行。 REF https://www.xiaoz.me/archives/14458 https://segmentfault.com/a/1190000014740871 https://zh.opensuse.org/openSUSE:How_to_write_a_systemd_service ","date":"2020-12-01","objectID":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/:4:0","series":null,"tags":[],"title":"如何编写一个 Linux Systemd Service？","uri":"/2020/12/01/%E5%A6%82%E4%BD%95%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA-linux-systemd-service/"},{"categories":["Linux"],"content":"连接国外或内网 centos7 主机时发现会因为 DNS 的问题造成 SSH 连接速度慢。 SSH 登录太慢可能是 DNS 解析的问题，默认配置下 sshd 初次接受 ssh 客户端连接的时候会自动反向解析客户端 IP 以得到 ssh 客户端的域名或主机名。如果这个时候 DNS 的反向解析不正确，sshd 就会等到 DNS 解析超时后才提供 ssh 连接，这样就造成连接时间过长、ssh 客户端等待的情况，一般为 10-30 秒左右。有个简单的解决办法就是在 sshd 的配置文件（sshd_config）里取消 sshd 的反向 DNS 解析。 编辑 ssh 配置文件 vi /etc/ssh/sshd_config 找到 UseDNS 设置为 no 重启 ssh 服务即可 systemctl restart sshd ","date":"2020-11-28","objectID":"/2020/11/28/centos-ssh-%E7%99%BB%E5%BD%95%E5%A4%AA%E6%85%A2%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/:0:0","series":null,"tags":[],"title":"CentOS SSH 登录太慢的解决方法","uri":"/2020/11/28/centos-ssh-%E7%99%BB%E5%BD%95%E5%A4%AA%E6%85%A2%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"},{"categories":["Python"],"content":"读取本地文件内容，对文本内容进行中文分词，统计词频后，生成词云图。 一、生成矩形颜色随机的词云图 读取本地文件内容，对文本内容进行中文分词，统计词频后，生成矩形随机颜色的词云图。 import logging import collections import re import math import jieba from wordcloud import WordCloud jieba.setLogLevel(logging.INFO) # 创建停用词列表 def stopwordslist(): # 按行读入 stopwords = [line.strip() for line in open('chinsesstop.txt', encoding='UTF-8').readlines()] # 分割为单个字符（列表解析） stopwords = [k for s in stopwords for k in s] return stopwords # # 指定字符串方式 # text = \"collections在python官方文档中的解释是High-performance container datatypes，直接的中文翻译解释高性能容量数据类型。它总共包含五种数据类型\" # 文件读取方式 f = open(\"./article.txt\", \"r\", encoding=\"utf-8\") text = f.read() f.close() # 生成词云的词频限制，选取前30% TopWordFrequencyPercentage = 30 TopWordFrequencyPercentage /= 100 # 词云图片生成路径（当前目录下的 wordcloud_rectangle.png 文件） PicSavePath = \"./wordcloud_rectangle.png\" # jieba分词 seg = jieba.cut(text) seg = \" \".join(seg) seg = seg.strip() # 去除标点符号 seg = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:：。？、~@#￥%……\u0026*（）]+\", \" \", seg) # 只取中文 # seg = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', seg) # 转换为list seg = seg.split(\" \") # 过滤空字符和None seg = list(filter(None, seg)) # print(seg) # 创建一个停用词列表 stopwords = stopwordslist() # print(stopwords) # 过滤停用词（列表解析） seg = [v for v in seg if v not in stopwords] # print(seg) # 统计词频 word_counts = collections.Counter(seg) # print(word_counts) # print(math.ceil(len(word_counts) * 0.3)) # # 获取词频降序排列的前30% # word_counts_top_50_percent = word_counts.most_common(math.ceil(len(word_counts) * 0.3)) # print(word_counts_top_50_percent) # 生成词云 wc = WordCloud( # 限制词数（根据词频限制，计算个数，向上取整） max_words=math.ceil(len(word_counts) * TopWordFrequencyPercentage), # 设置背景宽 width=500, # 设置背景高 height=350, # 最大字体 max_font_size=50, # 最小字体 min_font_size=10, # 设置字体文件路径，不指定就会出现乱码。 font_path='./MSYH.TTC', # 设置背景色 background_color='white', ) # 根据词频产生词云 wc.generate_from_frequencies(word_counts) # 生成词云图片文件 wc.to_file(PicSavePath) 效果图： 二、根据图片生成规定形状的颜色相近的词云图 读取本地文件内容，对文本内容进行中文分词，统计词频后，根据背景图片生成规定形状和颜色的词云图。 import logging import collections import re import math from random import randint import jieba from wordcloud import WordCloud from wordcloud import ImageColorGenerator from PIL import Image import numpy as np jieba.setLogLevel(logging.INFO) # 创建停用词列表 def stopwordslist(): # 按行读入 stopwords = [line.strip() for line in open('chinsesstop.txt', encoding='UTF-8').readlines()] # 分割为单个字符（列表解析） stopwords = [k for s in stopwords for k in s] return stopwords # 自定义颜色函数（在绘制词云图时发现有的字颜色为黄色导致看不清因此需要修改整个词云图的色调为冷色调 蓝绿色） def random_color_func(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None): # what is HSL? https://baike.baidu.com/item/HSL/1443144?fr=aladdin H = randint(120, 250) S = int(100.0 * 255.0 / 255.0) L = int(100.0 * float(randint(60, 120)) / 255.0) return \"hsl({}, {}%, {}%)\".format(H, S, L) # # 指定字符串方式 # text = \"collections在python官方文档中的解释是High-performance container datatypes，直接的中文翻译解释高性能容量数据类型。它总共包含五种数据类型\" # 文件读取方式 f = open(\"./article.txt\", \"r\", encoding=\"utf-8\") text = f.read() f.close() # 生成词云的词频限制，选取前30% TopWordFrequencyPercentage = 30 TopWordFrequencyPercentage /= 100 # 词云图片生成路径（当前目录下的 wordcloud_arbitrary_shape.png 文件） PicSavePath = \"./wordcloud_arbitrary_shape.png\" # jieba分词 seg = jieba.cut(text) seg = \" \".join(seg) seg = seg.strip() # 去除标点符号 seg = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:：。？、~@#￥%……\u0026*（）]+\", \" \", seg) # 只取中文 # seg = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', seg) # 转换为list seg = seg.split(\" \") # 过滤空字符和None seg = list(filter(None, seg)) # print(seg) # 创建一个停用词列表 stopwords = stopwordslist() # print(stopwords) # 过滤停用词（列表解析） seg = [v for v in seg if v not in stopwords] # print(seg) # 统计词频 word_counts = collections.Counter(seg) # print(word_counts) # print(math.ceil(len(word_counts) * 0.3)) # # 获取词频降序排列的前30% # word_counts_top_50_percent = word_counts.most_common(math.ceil(len(word_counts) * 0.3)) # print(word_counts_top_50_percent) # 词云形状 mask = np.array(Image.open(\"./background1.png\")) # 根据图片颜色设置词云颜色(如果背景图片是纯色背景会报 NotImplementedError: Gray-scale images TODO 错","date":"2020-11-18","objectID":"/2020/11/18/python%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A0%B9%E6%8D%AE%E8%AF%8D%E9%A2%91%E5%92%8C%E8%83%8C%E6%99%AF%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91/:0:0","series":null,"tags":[],"title":"Python 中文分词，根据词频和背景图片生成词云","uri":"/2020/11/18/python%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%A0%B9%E6%8D%AE%E8%AF%8D%E9%A2%91%E5%92%8C%E8%83%8C%E6%99%AF%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%8D%E4%BA%91/"},{"categories":["Python"],"content":"有时 pip 不指定源安装会比较慢，甚至会安装失败。 #阿里源 pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ #豆瓣 pip install -r requirements.txt -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com #清华大学 pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/ ","date":"2020-11-17","objectID":"/2020/11/17/python-pip%E6%8C%87%E5%AE%9A%E6%BA%90%E5%AE%89%E8%A3%85/:0:0","series":null,"tags":[],"title":"Python pip 指定源安装","uri":"/2020/11/17/python-pip%E6%8C%87%E5%AE%9A%E6%BA%90%E5%AE%89%E8%A3%85/"},{"categories":["Python"],"content":"在查看别人的 Python 项目时，经常会看到一个 requirements.txt 文件，里面记录了当前程序的所有依赖包及其精确版本号，与 npm 的 package.json 很像。其作用是用来在另一台 PC 上重新构建项目所需要的运行环境依赖。 # 导出依赖 pip freeze \u003e requirements.txt # 安装依赖 pip install -r requirements.txt ","date":"2020-11-17","objectID":"/2020/11/17/python-%E5%AF%BC%E5%87%BA-or-%E5%AE%89%E8%A3%85-requirements.txt-%E4%BE%9D%E8%B5%96/:0:0","series":null,"tags":[],"title":"Python 导出 or 安装 requirements.txt 依赖","uri":"/2020/11/17/python-%E5%AF%BC%E5%87%BA-or-%E5%AE%89%E8%A3%85-requirements.txt-%E4%BE%9D%E8%B5%96/"},{"categories":["Python"],"content":"安装完 anaconda 后，打开 Anaconda Powershell Prompt (anaconda3) 执行命令。 一、Anaconda # 查看 conda 版本号 conda --version # 查看系统当前已有的 Python 环境 conda info --envs # 添加一个名为 python27，Python 版本为 2.7 的环境 conda create --name python27 python=2.7 # 查看当前环境的 Python 版本 python --version # 切换 Python 环境到刚才新添加的 Python2.7 conda activate python27 # 切回原来的 Python 环境 conda deactivate python27 # 或 conda activate base # 删除 python27 这个环境 conda remove --name python27 --all # 添加清华源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes # 查看镜像列表 conda config --show channels # 删除某个镜像 conda config --remove channels #如：https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ # 包管理 conda install \u003c包名\u003e 安装指定包 conda remove \u003c包名\u003e 移除指定包 conda update \u003c包名\u003e 更新指定包 二、PyCharm ","date":"2020-11-13","objectID":"/2020/11/13/pycharm-%E4%BD%BF%E7%94%A8-anaconda-%E7%AE%A1%E7%90%86%E5%A4%9A%E7%89%88%E6%9C%AC-python-%E7%8E%AF%E5%A2%83/:0:0","series":null,"tags":[],"title":"PyCharm 使用 Anaconda 管理多版本 Python 环境","uri":"/2020/11/13/pycharm-%E4%BD%BF%E7%94%A8-anaconda-%E7%AE%A1%E7%90%86%E5%A4%9A%E7%89%88%E6%9C%AC-python-%E7%8E%AF%E5%A2%83/"},{"categories":["Python"],"content":"先说结论：推荐使用 openyxl 库，而不是 xlwt 库。使用 xlwt 库创建并向 .xls 文件写入内容时，其单个 sheet 限制最大行数为 65535。而使用 openpyxl 库创建并向 .xlsx 文件写入内容时，其单个 sheet 限制最大行数为 1048576，最大列数为 16384。 一、openpyxl 库 openyxl 库文档地址：https://openpyxl.readthedocs.io/en/stable/usage.html / https://pypi.org/project/openpyxl import openpyxl ExeclFileSavePath = \"./test.xlsx\" wb = openpyxl.Workbook() rowInit = 1 rowLimit = 50 columnInit = 1 columnLimit = 5 # 默认Sheet ws = wb.active for row in range(rowInit, rowLimit + 1): for column in range(columnInit, columnLimit + 1): ws.cell(row, column).value = row # 创建一个sheet Pi wsA = wb.create_sheet(title=\"Pi\") for row in range(rowInit, rowLimit): wsA.append(range(columnInit, columnLimit)) # 保存文件 wb.save(ExeclFileSavePath) 二、xlwt 库 xlwt 库文档地址：https://xlwt.readthedocs.io/en/latest / https://pypi.org/project/xlwt import xlwt ExeclFileSavePath = \"./test.xls\" workbook = xlwt.Workbook(encoding='utf-8') sheet1 = workbook.add_sheet('sheet1', cell_overwrite_ok=True) # 给excel文件添加sheet rowInit = 1 rowLimit = 50 columnInit = 1 columnLimit = 5 for row in range(rowInit - 1, rowLimit): for column in range(columnInit - 1, columnLimit): temp = row # 单元格内容 sheet1.write(row, column, temp) # sheet1.write_merge(0, 3, 1, 1, '合并') # 合并单元格 workbook.save(ExeclFileSavePath) ","date":"2020-11-11","objectID":"/2020/11/11/python-%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-excel-%E6%96%87%E4%BB%B6%E5%B9%B6%E5%90%91%E5%85%B6%E5%86%99%E5%85%A5%E5%86%85%E5%AE%B9/:0:0","series":null,"tags":[],"title":"Python 如何创建一个 Excel 文件并向其写入内容？","uri":"/2020/11/11/python-%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA-excel-%E6%96%87%E4%BB%B6%E5%B9%B6%E5%90%91%E5%85%B6%E5%86%99%E5%85%A5%E5%86%85%E5%AE%B9/"},{"categories":["MySQL"],"content":"隐藏在各项配置文件之后的，是 MySQL 的思想和设计方案。 查看当前所有配置 showvariables;一、表相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:0:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"表数据独立存储 表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的，从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。 我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。 因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop/truncate table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 innodb_file_per_table = ON 二、日志相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:1:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"bin log 格式 如果是使用 delete 语句误删了数据行，可以用 Flashback 工具通过闪回把数据恢复回来。Flashback 恢复数据的原理，是修改 binlog 的内容，拿回原库重放。而能够使用这个方案的前提是，需要确保以下两个配置。 binlog_format=row binlog_row_image=FULL ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:2:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"bin log 文件体积限制 / 自动过期 row 格式记录的数据更完整，在数据恢复方面有极大优势，但缺点是生成的日志量大。因此需要根据业务需求和服务器配置对 binlog 设置单个文件最大体积限制和保留时长的限制。 expire_logs_days = 30 max_binlog_size = 512M ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:3:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"双 “1” 配置 一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 redo log 用于保证 crash-safe 能力，innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后 redo log 数据不丢失。 sync_binlog = 1 innodb_flush_log_at_trx_commit = 1 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:4:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"redo log 文件体积 / 个数 利用 WAL 技术，MySQL 将随机写转换成了顺序写，大大提升了数据库的性能。但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些，造成性能的间歇性下降。 “redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。 一个内存配置为 128GB、innodb_io_capacity 设置为 20000 的大规格实例，正常会建议你将 redo log 设置成 4 个 1GB 的文件。 这两个属性是只读的，需要通过修改配置文件并重启 MySQL 生效。 innodb_log_file_size=512M innodb_log_files_in_group=2 三、buffer 相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:5:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"缓冲池大小 缓冲池内存大小不够，意味着可用的数据页少，脏页比例容易接近 75%，造成性能间歇性抖动。 innodb_buffer_pool_size=1G ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:6:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"join 联表查询缓冲池大小 join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。如果 join 语句较慢，有可能是因为缓冲池过小。 join_buffer_size=32M 四、硬盘 IO 相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:7:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"IO 读写能力 如果没能合理地设置 innodb_io_capacity 参数，会导致一些性能问题。比如 MySQL 的写入速度很慢，TPS 很低，但是数据库主机的 IO 压力并不大。 如果主机磁盘用的是 SSD，但是 innodb_io_capacity 的值设置的是 300。于是，InnoDB 认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能。 测试完成后可以得到服务器硬盘的 IOPS，根据这个值来设置：（200 是默认值） innodb_io_capacity = 200 centos 安装 fio，测试硬盘 io yum install -y libaio-devel fio cd /;mkdir test;cd /test;touch file;fio -filename=/test/file -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=4k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest windows 可以使用 AS SSD 软件测试 IOPS ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:8:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"脏页刷写连坐机制 一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个 “邻居” 也带着一起刷掉；而且这个把 “邻居” 拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的 “连坐” 机制，值为 0 时表示不找邻居，自己刷自己的。 找 “邻居” 这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。 而如果使用的是 SSD 这类 IOPS 比较高的设备的话，建议把 innodb_flush_neighbors 的值设置成 0： 因为这时候 IOPS 往往不是瓶颈，而 “只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。 innodb_flush_neighbors = 0 五、线程相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:9:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"并发线程上限数 通常情况下，我们建议把 innodb_thread_concurrency 设置为 64~128 之间的值。并发连接和并发查询并不是同一个概念，你在 show processlist 的结果里，看到的几千个连接，指的就是并发连接。而 “当前正在执行” 的语句，才是我们所说的并发查询。 并发连接数达到几千个影响并不大，就是多占一些内存而已。我们应该关注的是并发查询，因为并发查询太高才是 CPU 杀手。这也是为什么我们需要设置 innodb_thread_concurrency 参数的原因。 在线程进入锁等待以后，并发线程的计数会减一，也就是说等行锁（也包括间隙锁）的线程是不算在 128 里面的。MySQL 这样设计是非常有意义的。因为，进入锁等待的线程已经不吃 CPU 了；更重要的是，必须这么设计，才能避免整个系统锁死。 innodb_thread_concurrency=128 六、锁相关 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:10:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"自增锁模式 MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。 这个参数的值被设置为 0 时，表示采用之前 MySQL 5.0 版本的策略，即语句执行结束后才释放锁； 这个参数的值被设置为 1 时：普通 insert 语句，自增锁在申请之后就马上释放；类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放； 这个参数的值被设置为 2 时，所有的申请自增主键的动作都是申请后就释放锁。 在生产上，尤其是有 insert … select 这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置： innodb_autoinc_lock_mode=2 ，并且 binlog_format=row. 这样做，既能提升并发性，又不会出现数据一致性问题。 innodb_autoinc_lock_mode=2 七、SQL 部分 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:11:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"安全模式 为了防止忘记在 delete 或者 update 语句中写 where 条件、where 条件里面没有包含索引字段、没有加入 limit 限制引起的非安全 SQL 语句造成数据误删等，写入该配置后，这些非安全语句执行就会报错。 sql_safe_updates = ON 配置模板 innodb_file_per_table = ON binlog_format=row binlog_row_image=FULL expire_logs_days = 30 max_binlog_size = 512M sync_binlog = 1 innodb_flush_log_at_trx_commit = 1 innodb_log_file_size=512M innodb_log_files_in_group=2 innodb_buffer_pool_size=1G join_buffer_size=32M innodb_io_capacity = 300 innodb_flush_neighbors = 0 innodb_thread_concurrency=128 innodb_autoinc_lock_mode=2 ","date":"2020-11-07","objectID":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/:12:0","series":null,"tags":[],"title":"MySQL5.7 生产环境推荐参数配置","uri":"/2020/11/07/mysql5.7-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%8E%A8%E8%8D%90%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/"},{"categories":["MySQL"],"content":"在日常开发中遇到的一些问题。 一、查看数据库的各个表占用的文件大小 以查看 test 数据库为例： SELECTtable_schemaAS'数据库',table_nameAS'表名',engineAS'存储引擎',table_commentAS'备注',table_rowsAS'记录数',TRUNCATE(data_length/1024/1024,2)AS'数据大小(MB)',TRUNCATE(index_length/1024/1024,2)AS'索引大小(MB)'FROMinformation_schema.TABLESWHEREtable_schema='test'ORDERBYtable_rowsDESC;二、查找持续时间超过 60s 的事务 select*frominformation_schema.innodb_trxwhereTIME_TO_SEC(timediff(now(),trx_started))\u003e60三、查看当前线程处理情况 配套 kill 语句可以处理突发事件 showfullprocesslist;killId;四、优化表 optimizetable`table_name_A`;重新组织表数据和相关索引数据的物理存储，以减少存储空间，提高访问表时的 I/O 效率。 但此操作会锁表，需要避开业务高峰期。 五、重建表 altertableAengine=InnoDB;试想一下，如果你现在有一个表 A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。 显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表 A 的话，就会造成数据丢失。因此，在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。 而在 MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 我给你简单描述一下引入了 Online DDL 之后，重建表的流程： 建立一个临时文件，扫描表 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件； 用临时文件替换表 A 的数据文件。 可以看到，不同之处在于，由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。 DDL 之前是要拿 MDL 写锁的，这样还能叫 Online DDL 吗？ alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。 需要补充说明的是，上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗 IO 和 CPU 资源的。因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用 GitHub 开源的 gh-ost 来做。 另外，使用 alter table t engine=InnoDB 有可能会让一个表占用的空间反而变大： 1、就是这个表本身就已经没有空洞，比如说刚刚做过一次重建表操作； 2、在 DDL 期间，如果刚好有外部的 DML 在执行，这期间可能会引入一些新的空洞； 3、在重建表的时候，InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不是 “最” 紧凑的。 ","date":"2020-11-05","objectID":"/2020/11/05/mysql-%E7%9A%84%E5%B8%B8%E7%94%A8%E7%BB%B4%E6%8A%A4%E8%AF%AD%E5%8F%A5/:0:0","series":null,"tags":[],"title":"MySQL 的常用维护语句","uri":"/2020/11/05/mysql-%E7%9A%84%E5%B8%B8%E7%94%A8%E7%BB%B4%E6%8A%A4%E8%AF%AD%E5%8F%A5/"},{"categories":["MySQL"],"content":"MySQL 的四种事务隔离级别 / 如何切换 MySQL 的默认全局事务隔离级别 / 了解 session 和 global 关键字 一、查看当前 MySQL 版本号 selectversion();二、查看当前全局事务隔离级别 ","date":"2020-11-04","objectID":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:0:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["MySQL"],"content":"2.1、MySQL5.6 及其更早的版本 select@@global.tx_isolation;","date":"2020-11-04","objectID":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:1:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["MySQL"],"content":"2.2、MySQL5.7 及更高版本 select@@global.transaction_isolation;1、MySQL5.7 引入了 transaction_isolation 用来代替 tx_isolation，并在 MySQL8.0.3 去掉了 tx_isolation，在 MySQL5.7 及更高版本中建议使用 transaction_isolation 2、若要查看当前会话的事务隔离级别，可以去掉 global. 使用 SELECT @@transaction_isolation。同理，若只想针对当前 session 设置事务隔离级别，可将 global 关键字替换为 session 三、MySQL 的四个事务隔离级别 | 事务隔离级别 | 脏读 | 不可重复读 | 幻读 | | —- | —- | | 读未提交（read-uncommitted） | 是 | 是 | 是 | | 读提交（read-committed） | 否 | 是 | 是 | | 可重复读（repeatable-read） | 否 | 否 | 是 | | 串行化（serializable） | 否 | 否 | 否 | 四、修改 MySQL 全局默认事务隔离级别 ","date":"2020-11-04","objectID":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:2:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["MySQL"],"content":"4.1、MySQL5.6 及其更早的版本 setglobaltx_isolation='read-uncommitted';setglobaltx_isolation='read-committed';setglobaltx_isolation='repeatable-read';setglobaltx_isolation='serializable';","date":"2020-11-04","objectID":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:3:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["MySQL"],"content":"4.2、MySQL5.7 及更高版本 setglobaltransaction_isolation='read-uncommitted';setglobaltransaction_isolation='read-committed';setglobaltransaction_isolation='repeatable-read';setglobaltransaction_isolation='serializable';","date":"2020-11-04","objectID":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:4:0","series":null,"tags":[],"title":"MySQL 修改默认事务隔离级别","uri":"/2020/11/04/mysql-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["Nginx"],"content":"我们知道，在 nginx 的 if 中是不能写 limit_req 和 limit_conn 的。也就是说在 nginx 的配置文件中，我们无法通过 if 对请求参数做逻辑判断，从而实现对复杂请求参数的精准限流。 应用场景：针对某一个流量特别大的入口的某一个特定 GET 请求参数做限流，比如来自微信小程序的 API 请求入口（https://example.com/app/index.php?i=99\u0026v=9.9.9\u0026m=xxxx\u0026from=wxapp），我希望 from = wxapp 时进行限流，若 from 不等于 wxapp 则不进行限流。 二、使用 map 针对请求参数限流 在 nginx 的 http 部分写入： limit_req_zone $paramFrom zone=from:10m rate=30r/s; map $arg_from $paramFrom { wxapp wxapp; } 这里的 rate 我设为了每秒处理 30 个请求，如果 limit_req 没有设置 burst，则默认为 0。 rate 支持 r/s 和 r/m 两种模式。 在 server 部分写入： location ^~ /app/index.php { limit_req zone=from; #limit_rate 512k; #宝塔PHP文件处理 try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi-56.sock; fastcgi_index index.php; include fastcgi.conf; include pathinfo.conf; } 三、关于 Nginx 的 map 模块 使用 map 模块，可以根据一个或多个变量组合成一个新的变量。通过判断新的变量，我们可以处理非常复杂的业务逻辑。本文是以 map 模块用于限流为示例，简单展示一下 map 的使用方法。 四、原理 http_limit_req_module 和 http_limit_conn_module 两个模块是在 nginx 的 preaccess 阶段对请求做拦截，也就是说使用 limit_req_zone 或是 limit_conn_zone 对请求都是可以的。 但两者有本质上的区别，request 和 connection 是不同的，在实际应用中应注意区别。 connection 是连接，即常说的 tcp 连接，通过三次握手而建立的。 request 是指请求，即 http 请求，（注意，tcp 连接是有状态的，而构建在 tcp 之上的 http 却是无状态的协议）。 limit_req_zone 或是 limit_conn_zone 对请求的限制有效性取决于 key 的设计，通常使用 realip 模块获取到的客户端 IP。但 IP 是针对全体用户做的限流，显然不能满足我们的需求。所以我们需要使用 map 模块来匹配特定的请求参数生成一个变量，将这个变量作为 limit_req_zone 或是 limit_conn_zone 的 key，这样就可以实现我们的需求了。 ","date":"2020-09-17","objectID":"/2020/09/17/nginx-%E6%A0%B9%E6%8D%AE%E7%89%B9%E5%AE%9A%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E5%81%9A%E9%99%90%E6%B5%81/:0:0","series":null,"tags":["限流"],"title":"Nginx 根据特定请求参数做限流","uri":"/2020/09/17/nginx-%E6%A0%B9%E6%8D%AE%E7%89%B9%E5%AE%9A%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%E5%81%9A%E9%99%90%E6%B5%81/"},{"categories":["Nginx"],"content":"使用 nginx 的 http_limit_conn_module 模块可以在 nginx 的 preaccess 阶段对请求的并发做拦截。限制的有效性取决于 key 的设计，通常使用 realip 模块获取到的客户端 IP。 应用场景：假如你只希望限流用户的访问入口，但不希望管理后台也被纳入限流的范围内，因为在操作管理后台时，用户访问量激增，nginx 会频繁返回 503，那么管理后台将处于不可操作的状态。 一、编写 location 规则 假设我需要对 /app/index.php 这个路径做并发量的控制（例如：https://example.com/app/index.php?i=99\u0026t=0\u0026v=5.5.5\u0026from=wxap），首先要确定的事情就是编写的 location 规则是否能被正确匹配到。下面是 nginx location 的写法： location ^~ /app/index.php { return 404; } 加上这条 location 规则后，如果访问（https://example.com/app/index.php?i=99\u0026t=0\u0026v=5.5.5\u0026from=wxap）这样的链接，nginx 会直接返回 404 错误。那么说明这段 location 是能够匹配到的。 二、限制并发量 在 nginx 的 http 部分添加： limit_conn_zone $binary_remote_addr zone=perip:10m; limit_conn_zone $server_name zone=perserver:10m; $binary_remote_addr 是二进制格式的 IPv4 地址，这个 IP 是请求者的 IP。将远程客户端的 IP 地址作为 zone 的 key，目的是为了对单个客户端做并发限制。 $server_name 指的是当前站点的名称，将这个作为 zone 的 key，目的是为了对某个站点做总体的并发限制。 稍稍修改一下上面的 location 规则，设置为站点并发量为 300，单个 IP 并发量限制 25： location ^~ /app/index.php { limit_conn perserver 300; limit_conn perip 25; limit_rate 128k; #宝塔面板默认的 enable-php-56.conf 规则，对php文件的响应处理 try_files $uri =404; fastcgi_pass unix:/tmp/php-cgi-56.sock; fastcgi_index index.php; include fastcgi.conf; include pathinfo.conf; } limit_conn 的作用是，在某个 zone 下的并发限制为多少。 limit_rate 限制的是 nginx 向客户端传送响应的速率。 limit_conn 和 limit_req 不能设置在 if 指令中，所以如果针对不同的 URL 进行限流，只能通过不同的 location 实现。 limit_rate 可以在 if 指令中，可以使用 if 指令匹配 URL 实现不同 URL 的限流。 使用 jmeter 等相关并发测试工具可以测到，确实对 /app/index.php/ 请求做了并发量的限制，超出并发量的部分 nginx 会默认返回 503。 ","date":"2020-09-16","objectID":"/2020/09/16/nginx-%E9%92%88%E5%AF%B9%E7%89%B9%E5%AE%9A%E8%B7%AF%E5%BE%84%E6%88%96%E5%85%A5%E5%8F%A3%E6%96%87%E4%BB%B6%E9%99%90%E6%B5%81/:0:0","series":null,"tags":["限流"],"title":"Nginx 针对特定路径或入口文件限流","uri":"/2020/09/16/nginx-%E9%92%88%E5%AF%B9%E7%89%B9%E5%AE%9A%E8%B7%AF%E5%BE%84%E6%88%96%E5%85%A5%E5%8F%A3%E6%96%87%E4%BB%B6%E9%99%90%E6%B5%81/"},{"categories":["PHP","Redis"],"content":"ThinkPHP 如何使用 Redis 实现悲观锁解决高并发情况下读写带来的脏读问题 / ThinkPHP5.1 / Redis Cache / File Cache 测试。 在用户量 / 客户端数量比较少的时候，只要系统的业务逻辑是正确的，一般都不会发现有什么问题。但随着用户量 / 客户端数量逐渐增多，高并发带来的问题就会逐渐出现，而脏读是众多问题的其中之一。 一、无并发控制，会带来什么问题？ 本文以 ThinkPHP5.1.39 的代码作为案例，下面是一个 File Cache 读写操作： public function fileCacheCase(){ $keyName = \"test\"; $keyValue = 996; //写入缓存 Cache::set($keyName, $keyValue, 3600); //从缓存中获取值 $data = Cache::get($keyName); //删除缓存 Cache::rm($keyName); echo \"OK! $data\"; } 访问这个 function，会输出 OK! 996 。无论你访问几次，结果都是如此，但仅限于单线程的情况（只有你自己一个人在访问这个 function），如果是多个人同时不停的访问这个 function，还会是这样吗？想一想 😛 使用 jmeter 测试一下，120 线程测试了十几秒，发现了 3 种不同的返回结果。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:0:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"1.1、返回了 OK! 996 与单线程时的结果一致，是正常处理逻辑。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:1:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"1.2、只返回了 OK！而不是 OK! 996 说明缓存不存在，原因是：在 A 线程将 996 写入缓存后，B 线程将缓存删除了。此时 A 线程从缓存中读出来的数据为 null，所以 A 线程输出了 OK! ，而不是 OK! 996。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:2:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"1.3、返回了一个 500 错误 报错的内容是： file_get_contents（…）No such file or directory。 显然是 cache 文件夹下的某个缓存文件不存在，所以引起了这个错误。原因是：A 线程在删除缓存后，B 线程也在执行删除缓存的操作。当缓存文件已被删除时，再执行删除缓存文件的操作，自然就报了文件不存在的错误。（实测 120 个线程并发，总计 500 个请求，异常率 0.20%） 尽管我修改了 File Cache 的 133 行，在删除前判断文件是否存在，虽然异常率降低了，但依然无法从根本上解决问题。可以看到的是，在高并发场景下，问题已经显现出来了。 下面我们用 redis 缓存试试看： public function fileCacheCase(){ $keyName = \"test\"; $keyValue = 996; //写入缓存 Cache::store('redis')-\u003eset($keyName, $keyValue, 3600); //从缓存中获取值 $data = Cache::store('redis')-\u003eget($keyName); //删除缓存 Cache::store('redis')-\u003erm($keyName); echo \"OK! $data\"; } 经过测试，与上面的 3 种情况一致。（根据 thinkphp5.1 的官方文档，我使用的是 store 来切换到 redis，但不知道为何，仍然会报 File Cache 驱动的 No such file or directory/unlink 错误，十分诡异）。 如何解决高并发场景下带来的脏读问题？ 答案是：使用锁机制。 二、关于锁机制 根据锁的控制范围，可分为单机锁 / 分布式锁 2 种。根据锁的实现思想，可分为悲观锁 / 乐观锁 2 种。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:3:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"2.1、单机锁 即为单机环境的锁，无分布式设计。 常用的实现工具： Redis Memcached ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:4:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"2.2、分布式锁 为了防止分布式系统中的多个进程之间相互干扰，我们需要一种分布式协调技术来对这些进程进行调度。而这个分布式协调技术的核心就是来实现这个分布式锁。 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行 高可用的获取锁与释放锁 高性能的获取锁与释放锁 具备锁失效机制，防止死锁 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败 常用的实现工具： Zookeeper Redis Memcached Chubby ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:5:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"2.3、悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java 中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:6:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"2.4、乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和 CAS 算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java 中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:7:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"2.5、如何选择悲观 / 乐观锁？ 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 三、Redis 实现悲观锁 在商品秒杀活动活动中，流量峰值相对平常时的流量是高出非常多的。使用 Redis 实现悲观锁机制，可以解决商品库存脏读的问题。 初始化库存： public function stockInit() { $key = \"stock\"; $stockInit = 699; //清空所有缓存 Cache::clear(); Cache::store('redis')-\u003eclear(); //写入库存初始值 Cache::store('redis')-\u003eset($key, $stockInit); echo 'stock Init'; } ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:8:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"3.1、悲观锁实现（一）非最佳实践 看似符合逻辑的商品秒杀： public function flashSale() { $key = \"stock\"; $lockSuffix = \"_lock\"; //判断库存锁是否存在 while (Cache::get($key . $lockSuffix) == true) { // 存在锁定则等待 usleep(200000); } //库存上锁 Cache::store('redis')-\u003eset($key . $lockSuffix, 1, 30); //获取库存值 $stock = Cache::store('redis')-\u003eget($key); //减库存 if ($stock \u003e 0) { $temp = $stock; $stock -= 1; } else { //打开库存锁 Cache::store('redis')-\u003eset($key . $lockSuffix, false); return \"已售罄\"; } Cache::store('redis')-\u003eset($key, $stock); //打开库存锁 Cache::store('redis')-\u003eset($key . $lockSuffix, false); return \"恭喜，您抢到了第 {$temp}个库存！\"; } 实测 150 线程并发，异常率 0%，虽然引用了锁机制，看似符合逻辑的锁机制，但仍会有极低的概率脏读，原因无他，有 N 个线程同时抢到了锁。虽然概率低，但线程一多仍然会脏读。所以需要改用 redis 原生支持的 setnx 来保证只有一个线程抢到了锁。 如下，两个线程同时抢到了第 80 个库存： ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:9:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["PHP","Redis"],"content":"3.2、悲观锁实现（二） setnx 是 set if not exists 的简写，在 key 不存在时等价于 set，如果 key 存在，则不更新缓存内容，且返回 false。使用这个特性，可以保证锁只有一个线程抢到了。 使用 redis setnx 实现悲观锁的商品秒杀： public function flashSale() { $redisConifg = config('cache.redis'); //获取当前模块下的config文件夹中的cache文件的redis配置数组 $redis = Cache::connect($redisConifg); //获取thinkPHP官方封装的Redis Cache对象 $handler = Cache::connect($redisConifg)-\u003ehandler();//获取php redis扩展原生redis对象 https://github.com/phpredis/phpredis $key = \"stock\";//商品库存缓存名 $lockSuffix = \"_lock\";//商品库存锁后缀名 $timeOut = 10; //库存锁过期时间 //抢库存锁 while ($handler-\u003eset($key . $lockSuffix, 1, ['nx', 'ex' =\u003e $timeOut]) == false) { // 没有抢到则等待 usleep(20000); } //当前线程抢到库存锁了 //获取库存值 $stock = $redis-\u003eget($key); //减库存 if ($stock \u003e 0) { $temp = $stock; $stock -= 1; } else { //删除库存锁 $redis-\u003erm($key . $lockSuffix); return \"已售罄\"; } //更新库存值 $redis-\u003eset($key, $stock); //删除库存锁 $redis-\u003erm($key . $lockSuffix); return \"恭喜，您抢到了第 {$temp}个库存！\"; } 150 线程并发测试后，并没有发现有异常情况了。根据实际业务需求，可以增加等待超时机制。 四、REF https://redis.io/commands/set https://github.com/phpredis/phpredis#set https://www.jianshu.com/p/a1ebab8ce78a https://blog.csdn.net/qq_34337272/article/details/81072874 ","date":"2019-05-18","objectID":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/:10:0","series":null,"tags":["悲观锁"],"title":"ThinkPHP5.1 如何使用 Redis 实现悲观锁","uri":"/2019/05/18/thinkphp5.1-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-redis-%E5%AE%9E%E7%8E%B0%E6%82%B2%E8%A7%82%E9%94%81/"},{"categories":["闲聊"],"content":"关于内容和工具 大概在半年前，尝试并且使用了一些笔记软件，于是逐渐陷入寻找的沼泽里，一发不可收拾，开始体验各种笔记软件，这似乎是陷入了一个不应该存在的怪圈：找笔记APP，体验，找下一个笔记APP。而我们本心不该是这样的，本来是想找个称手的兵器打仗，最后却变成了武器商店的店长。 近期逐渐意识到了，应该逐渐回归初心，想到什么内容，就找合适的地方马上开始写，内容远比载体重要的多。 不再去把大量的时间花在寻找好工具上了，专注于内容产出，提升自己，显然更加重要。 ","date":"2019-05-18","objectID":"/2019/05/18/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%AE%A2/:0:0","series":null,"tags":[],"title":"关于博客","uri":"/2019/05/18/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%AE%A2/"}]